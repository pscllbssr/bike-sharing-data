---
title: "Analysis of norwegian bike sharing data"
author: "Elise Gourri, Emilia Marlene Ribeiro Peixoto, Pascal Albisser"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
options(knitr.table.format = function() {
  if (knitr::is_latex_output())
    "latex" else "pipe"
})

options(scipen = 999)
```

```{r libraries, message = FALSE, echo = FALSE}
# data loading
library(readr)
library(glue)
library(RCurl)

# data manipulation and visualization
library(data.table) # aggregation of large data
library(tidyverse)
library(lubridate)
library(kableExtra) # to render tables in html
library(gridExtra)
library(ggplot2)
library(dplyr)
library(scales)
library(zoo)

# machine learning libraries
library(e1071)
library(caret)
library(kernlab)
library(mgcv)
library(neuralnet)

# geo-packages
library("geosphere")
```


# Bike sharing in Norway

```{r picture, echo=FALSE, out.width="50%", fig.cap="Locations of the bike sharing systems", fig.align="center"}
knitr::include_graphics("data/map_norway.png")
```

This report focuses on the study of bike sharing systems in three cities in Norway: [Bergen](https://bergenbysykkel.no/en/open-data/realtime), [Oslo](https://oslobysykkel.no/en/open-data/realtime), and [Trondheim](https://trondheimbysykkel.no/en/open-data/realtime). The analysis is based on freely available data from the year 2022, encompassing trip details and station information. We also included [weather data](https://meteostat.net/en/).  
The bike sharing systems operate on a docked bike model, with bikes available at official stations and returned to other official stations.  
The inclusion of weather data allows for a comprehensive understanding of factors influencing bike usage. The study aims to demonstrate the application of machine learning techniques to improve bike-sharing systems' efficiency and planning.

Our dataset is composed of the main following variables:

| Field Name | Data Type | Description |
| :--- | :--- | :--- |
| started_at | DATETIME | Start of trip |
| ended_at | DATETIME | End of the trip |
| duration | INTEGER | Duration of trip in seconds |
| start_station_id | INTEGER | Identifier of start station |
| start_station_latitude | DECIMAL | Location latitude of start station |
| start_station_longitude | DECIMAL | Location longitude of start station |
| end_station_id | INTEGER | Identifier of end station |
| end_station_latitude | DECIMAL | Location latitude of end station |
| end_station_longitude | DECIMAL | Location longitude of end station |
| city | STRING | Name of city |
| temp | DECIMAL | Approximate temperature during the trip |
| dwpt | DECIMAL | Dew point in °C |
| prcp | DECIMAL | Total Precipitation in mm |

Our initial dataset looks as follows:

```{r load_data, cache=TRUE}
data_raw <- fread("data/trips_2022.csv")

data_raw <- data_raw %>% mutate(start_time = as.ITime(started_at)) %>%
  mutate(end_time = as.ITime(ended_at))

head(data_raw) %>% kbl() %>%  kable_classic(full_width = F, html_font = "Cambria")
```

# Data preparation

We needed to modify the dataset to be able to perform our analysis.

## Retain only data from April to November

In the cities of Bergen^[https://bergenbysykkel.no/en/faq] and Oslo^[https://oslobysykkel.no/en/how-it-works], bikes are available all year round.
In Trondheim, the bikes are available from April to the start of December^[https://trondheimbysykkel.no/en/faq]. 
We therefore focused our analysis on the months between April and November.

```{r filter_dates, results=FALSE}
# Create a new column with the month of each trip start date
data <- data %>%
  mutate(start_month = month(started_at))

# Count the number of rows in the original dataframe
before_date_filter <- nrow(data)

# Filter the trips for the months between April and November
data_filtered <- data %>%
  filter(start_month >= 4 & start_month <= 11)

# Count the number of rows in the filtered dataframe
after_date_filter <- nrow(data_filtered)

# Calculate the number of removed trips
row_removed_month <- before_date_filter - after_date_filter

data <- data_filtered
```
We removed `r row_removed_month` trips from our dataset.

We also added a column containing the day of the week as well as a column to indicate if the day belongs to the week-end. Official holidays are ignored for this analysis.

```{r weekday}
# Add a column for the day of the week
data$weekday <- weekdays(as.Date(data$started_at))
# Add a column
data$weekend <- ifelse(weekdays(as.Date(data$started_at)) %in% c("Saturday", "Sunday"), "Weekend", "Weekday")
```

## Duration of trips
```{r duration_nb, results=FALSE}
data <- data_raw

# count the number of trips that lasted 2 minutes or less and starting and ending at the same station
short_trips <- nrow(subset(data, duration <= 120 & start_station_id == end_station_id))
short_trips

# count the number of trips that lasted 24 hours and more
long_trips <- nrow(subset(data, duration >= 86400))
long_trips

# remove the data concerning the "short" and long "trips"
data <- data[!(data$duration <= 120 & data$start_station_id == data$end_station_id) & !(data$duration >= 86400), ]

# longest trip (under 24 hours)
max(data$duration)
```

We removed `r short_trips` trips which lasted 2 min or less and with the same start and end stations. They probably correspond to users picking up a bike and immediately returning it. None of the trips lasted more than 24 hours.

```{r duration_plot, eval=TRUE}
data %>%
  ggplot(mapping = aes(x = city, y = duration, colour = city)) +
    geom_violin() +
    scale_y_continuous(trans = 'log10') +
    labs(title = "Distribution of the trips duration", y = "Duration (log-scale)", x = "")
```

Most of the trips were rather short trips. Some trips lasted several hours, the longest being approximately 19 hours in Oslo.

## Number of stations
```{r num_stations, results=FALSE}
# Count the number of stations in each city
num_stations_per_city <- data %>%
  group_by(city) %>%
  summarize(num_stations = n_distinct(start_station_id)) %>%
  pull(num_stations)

# Store the number of stations per city in a separate variable
num_stations_bergen <- num_stations_per_city[1]
num_stations_oslo <- num_stations_per_city[2]
num_stations_trondheim <- num_stations_per_city[3]
```

The number of stations for each city is summarized in the following table:

City  | Number of stations
:-|:-
Bergen | `r num_stations_bergen`
Oslo | `r num_stations_oslo`
Trondheim | `r num_stations_trondheim`

```{r top_chunk, message=FALSE}
# Group the data by city and station ID, and calculate the number of trips
trips_by_station <- data %>%
  group_by(city, start_station_id) %>%
  summarize(num_trips = n())

# Sort the data by city and num_trips, and keep only the top n stations for each city
top_no <- 20

top_stations <- trips_by_station %>%
  arrange(city, desc(num_trips)) %>%
  group_by(city) %>%
  top_n(top_no)

# Filter the original dataset to keep only trips from the top stations
data_top_stations <- data %>%
  semi_join(top_stations, by = c("city", "start_station_id"))

before_top <- nrow(data)
after_top <- nrow(data_top_stations)
removed_top <- before_top - after_top

data <- data_top_stations
```

To narrow our analysis furthermore, we only considered the data coming from the top `r top_no` stations in each city.

We removed `r removed_top` trips from our dataset, which then contained `r after_top` trips.

## Missing weather data

Given the large number of observations still available, we decided to not take into account trips with missing temperature values.

```{r missing_temp, results=FALSE}
data_filtered <- data %>% filter(!is.na(temp))

# Calculate the number of removed trips
removed_temp <- nrow(data) - nrow(data_filtered)

data <- data_filtered
```
We removed `r removed_temp` trips with missing temperature data.

After verification on the [meteostat.net website] (https://meteostat.net/en/), it seems that missing precipitations data corresponds to a value of 0.
We modified our dataset accordingly.

```{r precipitation}
data$prcp <- ifelse(is.na(data$prcp), 0, data$prcp)
```

## Haversine distance

We added the haversine distance for each trip between the start and end stations. Of course, following a straight line on a bike trip in a city is impossible, as many buildings and other obstacles exist. However, this calculation, at least, approximates how far the stations are apart. 

```{r haversine}
data <- data %>% 
  mutate(h_distance = distHaversine(
    cbind(start_station_longitude, start_station_latitude),
    cbind(end_station_longitude, end_station_latitude))) 

head(data %>% 
  select(start_station_longitude, start_station_latitude, end_station_longitude, end_station_latitude, h_distance, duration)) %>% kbl() %>%  kable_classic(full_width = F, html_font = "Cambria")
```

# Data exploration

## Trips per city
```{r trips_per_city_histo}
data %>% 
  ggplot(mapping = aes(x = city, fill = city)) + 
    geom_bar() +
  geom_text(aes(label = after_stat(count)), stat = 'count', position = position_dodge(width = 1),
    vjust = -0.5, size = 4) +
  labs(title = "Number of Trips per City", y = '# trips', x = '') +
  guides(fill = 'none')   
```

```{r city_percent}
# Calculate the percentage of observations for each city
city_percentages <- data %>%
  group_by(city) %>%
  summarize(percentage = n() / nrow(data) * 100)

# Store the percentage for each city in separate variables
bergen_percentage <- city_percentages$percentage[city_percentages$city == "Bergen"]
trondheim_percentage <- city_percentages$percentage[city_percentages$city == "Trondheim"]
oslo_percentage <- city_percentages$percentage[city_percentages$city == "Oslo"]
```

The city with most of the trips in our dataset was Oslo (`r round(oslo_percentage,0)`% of the observations), followed by Bergen (`r round(bergen_percentage,0)`%). The data of the city of Trondheim represented `r round(trondheim_percentage,0)`% of our dataset.

```{r trips_per_month, message=FALSE}
data %>% 
  group_by(city, year_month = paste(format(started_at, "%Y-%m"), "-01", sep="")) %>% 
  summarise(n = n()) %>% 
  ggplot(mapping = aes(y = n, x = as.Date(year_month), group = city, colour = city)) + 
    geom_line() +
    geom_point() +
    labs(title = "Sum of Trips per City and Month", y = "Number of Trips", x = "")

# Extract the month from the "started_at" column
data$month <- month(data$started_at)

# Count the number of observations for each month
month_counts <- table(data$month)

# Find the month with the highest number of observations
max_month <- names(month_counts)[which.max(month_counts)]
```
The month with the most trips in our dataset was August.

## Time of the trips
```{r time_trips_plot, message=FALSE}
# Create a new variable for the hour of the day
data$Hour <- hour(data$started_at)

# Group the data by city and hour, and calculate the count of observations
hourly_counts <- data %>%
  group_by(city, Hour) %>%
  summarize(count = n())

# Create the plot
ggplot(hourly_counts, aes(x = Hour, y = count, color = city)) +
  geom_line() +
  geom_point() +
  labs(x = "Hour of the Day", y = "Number of Trips")

# Group the data by hour, and calculate the count of observations
hourly_counts2 <- data %>%
  group_by(Hour) %>%
  summarize(count = n())

# Find the three hours with the maximum count
top_hours <- hourly_counts2 %>%
  slice_max(order_by = count, n = 3) %>%
  pull(Hour)

# Store the three hours in separate variables
hour1 <- top_hours[1]
hour2 <- top_hours[2]
hour3 <- top_hours[3]
```
There are no missing values concerning the start or the end time of the trips.  
Our dataset contains bike trips throughout the day for almost every city. 
We could identify 2 peak times of usage: in the morning at `r hour2`am and in the afternoon between `r hour1`pm and `r hour3`pm.

## Price of the trips
```{r price_trips}
data$price <- ifelse(data$duration <= 3600, 0, (ceiling((data$duration - 3600) / 900)) * 15)
free_trips <- round(sum(data$duration <= 3600) / nrow(data) * 100,0)
```

We added a column containing the price per trip.
In the 3 cities Bergen, Oslo and Trondheim, the first 60 min are free, each 15 supplementary minutes cost NOK 15 ^[https://trondheimbysykkel.no/en/how-it-works].   
Most of the trips last under 60 min (`r free_trips`%) and are free (included in the basis subscription).

## Distance of the trips

We can also plot the distance in relation to duration of the trips.

```{r haversine_plot, warning=FALSE}
data %>% 
  sample_n(10000) %>% 
  ggplot(mapping = aes(x = h_distance, y = duration, color = city)) +
  geom_point(alpha = 1/2) +
  scale_y_continuous(trans = 'log10') + 
  scale_x_continuous(trans = 'log10') +
  labs(x = 'Haversine distance (m, log-scale)', y = 'Duration (s, log-scale)', title = 'Distance vs. time in bike trips')
```


## Weather

We looked at the evolution of the temperature and precipitations in Bergen, Oslo and Trondheim between April 2022 and November 2022.

### Temperature

```{r daily_temp, message=FALSE, warning=FALSE}
# Calculate the mean temperature per city and day
daily_temp <- data %>%
  mutate(started_day = as.Date(started_at)) %>%
  group_by(city, started_day) %>%
  summarize(mean_temp = mean(temp)) %>% 
  mutate(mean_temp_7d_avg = rollmean(mean_temp, k = 7, fill = NA))

# Plot temperature by city
ggplot(daily_temp, aes(x = started_day, y = mean_temp_7d_avg, color = city)) +
  geom_line() +
  labs(x = "Date", y = "Mean Temperature, 7-day rolling mean", title = "Mean Temperature by City") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r data_temp}
# Find the row with the maximum mean temperature
max_temp_row <- daily_temp[daily_temp$mean_temp == max(daily_temp$mean_temp), ]

# Extract the city and started_at information from the row
city_max_temp <- max_temp_row$city
day_max_temp <- max_temp_row$started_day
max_temp <- round(max_temp_row$mean_temp,1)

mean_temp_Bergen <- round(mean(daily_temp$mean_temp[daily_temp$city == "Bergen"]),1)
mean_temp_Oslo <- round(mean(daily_temp$mean_temp[daily_temp$city == "Oslo"]),1)
mean_temp_Trondheim <- round(mean(daily_temp$mean_temp[daily_temp$city == "Trondheim"]),1)
```

The maximum temperature was reached in `r city_max_temp` with `r max_temp`°C on `r day_max_temp`.
The mean temperature for each city is displayed in the following graph as a vertical line.

```{r temperature_trips}

line_dummy <- data.frame(city = c("Bergen", "Oslo", "Trondheim"), Z = c(mean_temp_Bergen, mean_temp_Oslo, mean_temp_Trondheim))

ggplot(data, aes(x = temp, fill = city)) +
  geom_histogram(binwidth = 1, color = "black", position = "identity", alpha = 1/2) +
  labs(x = "Temperature (°C)", y = "Number of Observations", title = "Number of Observations per Temperature") +
  facet_wrap(~ city) +
  geom_vline(data = line_dummy, aes(xintercept = Z), linetype = "dashed", linewidth = 1) +
  guides(fill = 'none') 
  
```

Most of the bike trips occurred when the temperature was above 10°C.

### Precipitations
```{r precipitations, message=FALSE}
# Calculate the mean precipitation per hour and city
hourly_prcp <- data %>%
  group_by(city, Hour = format(started_at, "%H"), date = as.Date(started_at)) %>%
  summarize(mean_prcp = mean(prcp))

# Calculate the sum precipitation per day and city
daily_prcp <- hourly_prcp %>%
  group_by(city, date) %>%
  summarize(sum_prcp = sum(mean_prcp))

# Plot precipitation by city
ggplot(daily_prcp, aes(x = date, y = sum_prcp, color = city)) +
  geom_line() +
  labs(x = "Date", y = "Total Precipitation", title = "Total Precipitation per Day by City") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Rain was common in the three cities between April and October 2022.

# Modeling of the trip duration

Understanding the parameters influencing the bike trip duration can be valuable for the bike sharing company in planning and optimizing their services for their users.
The haversine distance between start and end station was expected to be the main parameter influencing the duration of a trip but we discovered that other parameters also had an influence on the trip duration.

## Linear Model (Elise)

We first used a linear model using weather data (temperature, precipitation and dew point) as well as the day (weekday or weekend) to predict the duration of the trips.
```{r model_lm2}
# Fit a linear regression model with duration as the response variable and weekend, distance, temperature and precipitations as predictor variables
model_lm2 <- lm(log(duration) ~ h_distance + temp + prcp + dwpt + poly(temp, degree=2) + poly(prcp, degree=2) + poly(dwpt, degree=2)+ weekend, data = data)

# Summarize the results of the model
summary(model_lm2) 
```

All chosen predictor variables have a significant impact on the duration of the trips. The R^2 is of `r round(summary(model_lm2)$r.squared,2)` and RMSE is `r round(sqrt(mean(model_lm2$residuals^2)),2)`.   
The following plot represents the observed values for the duration of the trips versus values predicted by our linear model. The blue line corresponds to the identity between observed and predicted values.

```{r model_lm2_predictions}
# Create a data frame with the observed and predicted values
pred_df <- data.frame(duration = log(data$duration),
                       predicted = predict(model_lm2))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_df, aes(x = predicted, y = duration)) +
   geom_point(alpha = 1/10) +
   geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
   labs(x = "Predicted Duration (log)", y = "Observed Duration (log)")
```

## Generalised Additive Model (Emilia)

Another approach to predict the duration of the trips using the same variables as before was to use a Generalised Additive Model (GAM).
We first plotted the duration against the different predictors and added a smooth line.

```{r gam_param_lm, message=FALSE}
#retrieve only chosen parameters and rename the columns
gam_data <- data %>% select(c(duration, temp, prcp, dwpt, h_distance, weekend))
colnames(gam_data) <- c("Duration", "Temperature", "Precipitation", "Dew_Point", "Distance", "Weekend")

#display relationship of duration with the different variables
template.graph.weather <- ggplot(data = gam_data,
                                 mapping = aes(y = log(Duration))) +
  geom_point(alpha = 1/10) +
  geom_smooth()

plot1 <- template.graph.weather + aes(x = Temperature)
plot2 <- template.graph.weather + aes(x = Precipitation)
plot3 <- template.graph.weather + aes(x = Dew_Point)
plot4 <- template.graph.weather + aes(x = Distance)

grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)
```

The distribution of the data for the variables temperature and dew point is evenly scattered through the graph. Whereas, the precipitation values are mainly available on the left side of the graph area.
These variables seem to have a somewhat linear relationship with the duration of the trips that remains flat throughout the graph, therefore, an effect is difficult to spot here.
The relationship between the distance and the duration of the trip is not everywhere linear.

```{r gam_lm}
# Fit the gam model
model_gam <- gam(log(Duration) ~ s(Temperature) + s(Precipitation) + s(Dew_Point) + s(Distance) + Weekend,
              data = gam_data)

# Summarize the results of the model
summary(model_gam)
```

The summary output of the GAM model indicates that there is a strong evidence that the temperature, the precipitations, the dew point, and the distance have a strong non-linear effect on the duration of the trips. The distinction weekday/weekend also influences the duration of a bike trip.
The smooth term for the distance has the largest edf value (8.998) and precipitation the lowest with a value of 5.554. The intercept is existing.

We made similar observations with the more simplistic linear model used above. The GAM model performs slighlty better: the R^2 is of 0.39.

## Estimating the trip duration with a Neural Network (Pascal)

We also wanted to see if we could get a better estimation of the trip duration with a neural network. In order to do this, we first scaled our dataset and then splitted it in a test- and train-part. Round trips have been excluded from this analysis.

```{r prepare_neuronal_net_data}
# fix random generator
set.seed(6987)

# remove obvious round trips
data_trips <- data %>% 
  filter(start_station_id != end_station_id) 

# select the predictors
predictors <- c("h_distance", "temp", "prcp")

# select columns, drop NA and take random subsample of 100k trips
data_trips <- data_trips %>% 
  select(c("duration", all_of(predictors))) %>%
  drop_na() %>% 
  sample_n(10000) 

# scale
max <- apply(data_trips, 2, max)
min <- apply(data_trips, 2, min)
data_trips_scaled <- as.data.frame(scale(data_trips, center = min, scale = max - min))

# split train/test
trainIndex <- createDataPartition(
  y = data_trips_scaled$duration, 
  p = .8,
  list = FALSE,
)
train_trips <- data_trips_scaled[trainIndex,]
test_trips <- data_trips_scaled[-trainIndex,]
```

As a baseline model, we took a very simple network consisting of one predictor (the haversine distance) and one hidden layer containing a single neuron.

```{r nn_baseline}
# train the neural network
trips_net <- neuralnet(duration ~ h_distance, data = train_trips, linear.output = TRUE)

# predict results on test-set
prediction <- compute(trips_net, test_trips)

plot(trips_net, rep = "best")
```

The baseline already performed quite well, considering the obvious outliers of sightseeing trips:

```{r nn_baseline_evaluation}
# scale back
prediction_rescaled <- prediction$net.result * (max['duration'] - min['duration']) + min['duration']
duration_rescaled <- test_trips$duration * (max['duration'] - min['duration']) + min['duration']

# calculate RMSE
rmse <- sqrt(mean((test_trips$duration - prediction$net.result)^2))
rmse_rescaled <- sqrt(mean((duration_rescaled - prediction_rescaled)^2))

# plot prediction vs real
plot(duration_rescaled, prediction_rescaled, col='blue', pch=16, ylab = "predicted duration", xlab = "real duration", main = 'Predictions of Baseline model', log="xy")

# plot perfect prediction
abline(0,1)

# add RMSE to plot
mtext(paste("RMSE: ", round(rmse, digits = 3), " (", round(rmse_rescaled, 3), ")"), side = 3)
```

The RMSE calculated with the data scaled back lies at `r round(rmse_rescaled, 3)`. This was already better than the linear model above, although we have to consider that the training data consisted only of a subset and we filtered out round trips in this case.

We wanted to see if we could achieve better results with different parameters. We added supplementary predictors and used the caret-package to try out different options of the neuronal net.

```{r nn_tuning, cache=TRUE}

tune_grid <- expand.grid(.layer1=c(1:2), .layer2=c(0:2), .layer3=c(0))
train_control <- trainControl(
  method = 'repeatedcv',
  number = 5,
  repeats = 5,
  returnResamp = 'final'
)

models <- train(duration ~ .,
                data = train_trips,
                method = 'neuralnet',
                metric = 'RMSE',
                linear.output = TRUE,
                tuneGrid = tune_grid,
                trControl = train_control
)

# extract the best model (concerning RMSE)
trips_net_tuned <- models$finalModel

# predict results on test-set
prediction_tuned <- compute(trips_net_tuned, test_trips)

# plot the winner
plot(trips_net_tuned, rep = "best")
```

The predictions of the tuned model looked similar to the predictions of the baseline model at first sight.

```{r nn_tuned_evaluation}
# scale back
prediction_tuned_rescaled <- prediction_tuned$net.result * (max['duration'] - min['duration']) + min['duration']
duration_rescaled <- test_trips$duration * (max['duration'] - min['duration']) + min['duration']

# calculate RMSE
rmse_tuned <- sqrt(mean((test_trips$duration - prediction_tuned$net.result)^2))
rmse_tuned_rescaled <- sqrt(mean((duration_rescaled - prediction_tuned_rescaled)^2))

# plot prediction vs real
plot(duration_rescaled, prediction_tuned_rescaled, col='blue', pch=16, ylab = "predicted duration", xlab = "real duration", main = 'Predictions of tuned model', log="xy")

# plot perfect prediction
abline(0,1)

# add RMSE to plot
mtext(paste("RMSE: ", round(rmse_tuned, digits = 3), " (", round(rmse_tuned_rescaled, 3), ")"), side = 3)
```

By adding more predictors, layers and neurons the RMSE decreased only a little bit, from `r round(rmse_rescaled, 3)` to `r round(rmse_tuned_rescaled, 3)`. This shows that the haversine distance still is the main predictor for the duration of a trip. Therefore, we can use the simple baseline model.

```{r nn_regression_example}

# predict 100m
predict_reg <- compute(trips_net, as.data.frame(scale(data.frame(h_distance = c(0, 1000)), center = min['h_distance'], scale = max['h_distance'] - min['h_distance'])))

predict_reg <- predict_reg$net.result * (max['duration'] - min['duration']) + min['duration']

```

According to the model, a biker needs roughly `r round(predict_reg[2,1])` seconds or ~`r round(predict_reg[2,1] / 60)` minutes for 1km airline distance, including the time needed for setting up and parking the bike at the end which the model estimates to be be at `r round(predict_reg[1,1])` seconds or ~`r round(predict_reg[1,1] / 60)` minutes.

# Modeling the number of outgoing trips with a Generalised Linear Model with family set to Poisson (Pascal)

We wanted to predict the number of available bikes needed at a given station depending on the temperature. As this is count data, we used a Generalised Linear Model with the family Poisson to achieve this.  

Some further data preparation was needed. We calculated the number of outgoing trips at each station and kept the 6 stations with the most trips. As there was as significant drop in July we also took into account the summer vacations between June 18th and August 21th.

```{r data_GLM_Poisson, message=FALSE}
# Group the data by city and station ID, and calculate the number of trips
trips_by_station <- data_raw %>%
  group_by(city, start_station_id) %>%
  summarise(num_trips = n(), 
            city = last(city)) %>% 
  arrange(desc(num_trips)) 

# show stations with most trips
head(trips_by_station, 6) %>% kbl() %>%  kable_classic(full_width = F, html_font = "Cambria")
```

```{r outgoing_trips, message=FALSE}
# limit to top 6 stations
top_station_ids <- trips_by_station$start_station_id[1:6] 
summer_vacation_int <- interval(ymd("2022-06-18"), ymd("2022-08-21"))

trips_per_station <- data %>% 
  filter(start_station_id %in% top_station_ids) %>% 
  mutate(started_trip_date = as.Date(started_at)) %>% 
  group_by(start_station_id, started_trip_date) %>% 
  summarise(
      trips = n(),
      avg_temp = mean(temp, na.rm = T), # beware: this is not average temperature of the day, but avg temperature of trips
      max_temp = max(temp, na.rm = T),
      min_temp = min(temp, na.rm = T),
      avg_prcp = mean(prcp, na.rm = T)
    ) %>% 
  mutate(
      month = month(started_trip_date),
      weekday = lubridate::wday(started_trip_date, week_start = 1, label = T),
      weekend = lubridate::wday(started_trip_date, week_start = 1) >= 5,
      summer_vacation = started_trip_date %within% summer_vacation_int
    )
```

We plotted the correlation between the average daily temperature at the start of the trips at each station and the number of trips.

```{r glm_plot, message=FALSE}
trips_per_station %>%
  ggplot(mapping = aes(y = trips, x = avg_temp)) + 
  facet_wrap(~start_station_id) + 
  geom_point() +
  stat_smooth(method = "glm", method.args = list(family = 'poisson'), colour = '#F8766D') +
  labs(title = "Number of Trips vs. Average Temperature", 
       subtitle = 'At 6 most frequented stations',
       y = "Number of trips", 
       x = "Avg. temperature C°")
```

We ran the model over the six stations.
```{r glm_poisson_temp}
trips_per_temp_glm <- glm(trips ~ avg_temp,
                     family = "poisson",
                     data = trips_per_station)
summary(trips_per_temp_glm)
```

```{r glm_poisson_coef}
glm_temp_coef <- exp(coef(trips_per_temp_glm)['avg_temp'])
```

As seen in the model output above, the temperature has a significant influence on the trips per day. For every increase of 1°C, we get a `r round(glm_temp_coef,2)` times increase in the number of trips a day.

# Analyzing the round trips with a Generalised Linear Model with family set to Binomial

```{r round_trip}
#new BOOLEAN variable: round_trip
data <- data %>% 
  mutate(round_trip = start_station_id == end_station_id)

# Count the proportions of observations with round_trip as TRUE
percent_round_trip <- round((sum(data$round_trip == TRUE) / nrow(data)) * 100,0)
```

We introduced a new variable in our dataset to analyze round trips: trips in which the user take and return a bike at the same station. Round trips represent `r percent_round_trip`% of our dataset. 

```{r round_plot}
data %>% 
  ggplot(mapping = aes(x = factor(start_station_id), fill = round_trip)) +
    geom_bar()
```

We use a generalized linear model with family set to binomial to analyze how the different cities, the distinction between weekday or weekend and the temperature influence the likelihood of a trip to be a roundtrip.
```{r binomial_round}
glm_binomial <- glm(round_trip ~ city - 1 + weekend + temp,
                     family = "binomial",
                     data = data)

summary(glm_binomial)
```

```{r binom_weekend}
#extraction of the coefficient "weekend" for interpretation of the model
exp_coef_weekend <- exp(coef(glm_binomial)["weekendWeekend"])
```

All variables present to have significant coefficients, consequently, they all highly impact the probability of a trip being a round trip. However, if we interpret individual coefficients such as the weekend/weekday, we can determine that the odds of a round trip on a weekend is `exp(Estimate(weekendWeekend)=` 1.94 higher than on weekday.

```{r binom_temp}
#extraction of the coefficient temperature
exp.coef.temp <- exp(coef(glm_binomial)["temp"])
#print(exp.coef.temp, digits = 3)

exp.coef.temp.10 <- exp(coef(glm_binomial)["temp"] * 10)
#print(exp.coef.temp.10, digits = 3)
```

When increasing the temperature by 1°C, the odds of a round trip increase by `exp(Estimate(temp)=` 4%.
To check the effect of the change of temperature, we wanted to test what would happen if we increase the temperature by 10°C. As we can see, the odds of a round trip increase to `exp(Estimate(weekendWeekend)*10=` 55%.

# Identifying cities with a Support Vector Machine

We tested if a Support Vector Machine could help predict the city based on the latitude and longitude of the stations.
This could for example be useful to adopt a nation-wide bike-sharing system without relying on station ID to identify the city of the station.

```{r cities_dataset}
# prepare data
cities_start <- data %>% select(c(start_station_latitude, start_station_longitude, city))
colnames(cities_start) <- c("latitude", "longitude","city")

cities_end <- data %>% select(c(end_station_latitude, end_station_longitude, city))
colnames(cities_end) <- c("latitude", "longitude","city")

cities <- rbind(cities_start, cities_end)
rm(cities_start, cities_end)

cities$city <- as.factor(cities$city)


set.seed(10)
cities_subset <- rbind(
  cities[city == 'Trondheim'][sample(nrow(cities[city == 'Trondheim']), 33300), ],
  cities[city == 'Oslo'][sample(nrow(cities[city == 'Oslo']), 33300), ],
  cities[city == 'Bergen'][sample(nrow(cities[city == 'Bergen']), 33300), ]
)
cities_subset %>% 
  group_by(city) %>% 
  summarise('number' = n()) %>% kbl() %>%  kable_classic(full_width = F, html_font = "Cambria")
```

```{r svm_plot}
cities_subset %>% 
  ggplot(mapping = aes(x = longitude, y = latitude, color = city)) +
    geom_point() +
    labs(title = "Start/End points per city")
```

```{r cities_dataset_split, warning=FALSE}
# form training and testing datasets
set.seed(10)
indices <- createDataPartition(cities_subset$city, p=0.7, list=F)

train_cities <- cities_subset %>% slice(indices)
test_cities <-  cities_subset %>% slice(-indices)
test_cities_truth <- cities_subset %>% slice(-indices) %>% pull(city)
```

```{r SVM_city_linear}
cities_svm <- svm(city ~. , train_cities, kernel = "linear", scale = TRUE, cost = 10)

#make predictions
test_pred <- predict(cities_svm, test_cities)
table(test_pred) %>% kbl() %>%  kable_classic(full_width = F, html_font = "Cambria")
```

```{r plot_linear_svm, cache=TRUE}
plot(cities_svm, train_cities, latitude ~ longitude)
```

```{r evaluate_linear_svm}
#evaluate the results
conf_matrix <- confusionMatrix(test_pred, test_cities_truth)
conf_matrix
```

No data points was misclassified. We built a SVM with a prediction accuracy of 100%.

# Conclusion

Our analysis focused on data obtained from the cities of Bergen, Oslo, and Trondheim in Norway. The project's initial phase involved data preparation to facilitate its utilization in the subsequent models we employed.

To begin with, we employed a linear model to examine the potential impact of variables such as the weekend, temperature, distance, and precipitation on the duration of bike rides. However, due to the model's inadequate fit, we employed a GAM using the same variables. The results from the GAM model substantiated our initial suspicions derived from the linear model. As the R-squared value increased in the GAM model, the reliability of the results improved. We further applied a simple neural net to estimate trip duration, which proved very useful. We hope these findings can give the bike rental company a comprehensive understanding of the factors influencing their business, aiding them in their strategic decisions. Since revenue is generated only after 60 minutes of bike riding, identifying the factors potentially affecting profitability becomes highly relevant for the company.

Furthermore, we wanted to provide the company with information enabling them to ascertain the demand for bikes. For this purpose, we employed a GLM with family set to Poisson. The results indicated a positive correlation between temperature and bike demand, implying that higher temperatures lead to higher usage of bikes. This information helps avoid shortages and accurately predict the required number of parking slots at bike stations, thereby maximizing profit and reducing costs.

Moreover, we sought to determine whether factors such as weekends or temperature influenced whether bike users would return to the same station from which they initially rented the bike. The results indicated that these factors do indeed impact return behavior. Once again, this information can serve as an additional metric for estimating demand on specific weekdays.

At last, we used a Support Vector Machine to determine the city based on latitude and longitude. Because the cities are considerably far apart, the classification worked very well. Such a model allows us to be independent of possibly costly location services/APIs and determine the location of a trip solely based on GPS data from the device itself.

Overall, our analysis provides valuable insights for the bike rental company, shedding light on the factors affecting ride duration, bike demand, and return behavior. With this knowledge, the company can make informed decisions to optimize operations, enhance customer satisfaction, and maximize profitability.