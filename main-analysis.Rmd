---
title: "Analysis of bike sharing data"
author: "Elise Gourri, Emilia Marlene Ribeiro Peixoto, Pascal Albisser"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: TRUE
    code_folding: hide
editor_options:
  chunk_output_type: console
---


```{r libraries, message = FALSE, echo = FALSE}

# data manipulation and visualization
library(data.table) # aggregation of large data
library(tidyverse)
library(lubridate)
library(kableExtra) # to render tables in html
library(gridExtra)
library(ggplot2)
library(dplyr)
library(scales)

# machine learning libraries
library(e1071)
library(caret)
library(kernlab)
library(mgcv)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = function() {
  if (knitr::is_latex_output())
    "latex" else "pipe"
})
```

# Dataset

Intro about the dataset:
-bike sharing system in Norway
-since when? how many cities?
-a small word concerning how the payment work and thus the usage of the bikes.
-system of docked bikes: bikes are available at official stations and have to be returned to an official station.
Data is available... we focus in the 3 cities Bergen, Oslo and Trondheim.

__Map of Norway with the bike sharing logos indicating each city.__

Our Dataset, pls run `scripts/download-data.R` first.

```{r head_data, cache=TRUE}
#data <- fread("data/trips.csv")
# We limit our analysis to data in 2022
data <- trips %>% filter(year(started_at) == 2022)
kable(head(data))
```

__Main variables:__

| Field Name | Data Type | Description |
| --- | --- | --- |
| started_at | DATETIME | Start of trip |
| ended_at | DATETIME | End of the trip |
| duration | INTEGER | Duration of trip in seconds |
| start_station_id | INTEGER | Identifier of start station |
| start_station_latitude | DECIMAL | Location latitude of start station |
| start_station_longitude | DECIMAL | Location longitude of start station |
| end_station_id | INTEGER | Identifier of end station |
| end_station_latitude | DECIMAL | Location latitude of end station |
| end_station_longitude | DECIMAL | Location longitude of end station |
| city | STRING | Name of city |
| temp | DECIMAL | Approximate temperature during the trip |
| dwpt | DECIMAL | Dew point in Â°C |
| prcp | DECIMAL | Total Precipitation in mm |
| snow | INTEGER | Snow Depth in mm |
| wpgt | DECIMAL | Wind Peak Gust in km/h |
| tsun | BOOLEAN | Total sunshine duration in minutes |
| coco | INTEGER | Weather Condition Code from 1 (clear) to 27 (storm) |

__add the following if used__
WDIR	Wind (From) Direction
WSPD	Average Wind Speed
RHUM	Relative Humidity
PRES	Sea-Level Air Pressure

#

Details for weather variables: <https://dev.meteostat.net/formats.html#time-format>

## Data preparation

We need to modify the dataset to be able to perform our analysis.

### Duration of trips -TO RUN-

```{r duration_nb, results=FALSE}
# count the number of trips that lasted 2 minutes or less and starting and ending at the same station
short_trips <- nrow(subset(data, duration <= 120 & start_station_id == end_station_id))
short_trips

# count the number of trips that lasted 24 hours and more
long_trips <- nrow(subset(data, duration >= 86400))
long_trips

# remove the data concerning the "short" and long "trips"
data <- data[!(data$duration <= 120 & data$start_station_id == data$end_station_id) & !(data$duration >= 86400), ]

# longest trip (under 24 hours)
max(data$duration)
```

The duration of each trips is indicated in seconds.

We removed `r short_trips` trips which lasted 2 min or less with the same start and end station. They probably correspond to users picking up a bike and immediately returning it. No trips lasted more than 24 hours.

__box plot is not the best... we need to do it better__
```{r duration_plot, eval=FALSE}
data %>%
  ggplot(mapping = aes(x = city, y = duration)) +
    geom_boxplot() +
    labs(title = "Distribution of the trips duration")
```
Most of the trips were rather short trips. Some trips lasted several hours, the longest being approximately 19 hours Oslo.

### Price of the trips

We add a column containing the price per trip.
In the 3 cities Bergen, Oslo and Trondheim, the first 60 min are free, each 15 supplementary minutes cost NOK 15 ^[https://trondheimbysykkel.no/en/how-it-works].

```{r price_trips}
data$price <- ifelse(data$duration <= 3600, 0, (ceiling((data$duration - 3600) / 900)) * 15)
```

### Retain only April to November -TO RUN-

In Trondheim, the bikes are available as long as there is no ice on the ground,
from April to the start of December. ^[https://trondheimbysykkel.no/en/faq]
We therefore focus our analysis on the months between April and November.

```{r filter_dates, results=FALSE}
# Create a new column with the month of each trip start date
data <- data %>%
  mutate(start_month = month(started_at))

# Count the number of rows in the original dataframe
before_date_filter <- nrow(data)
before_date_filter

# Filter the trips for the months between April and November
data_filtered <- data %>%
  filter(start_month >= 4 & start_month <= 11)

# Count the number of rows in the filtered dataframe
after_date_filter <- nrow(data_filtered)
after_date_filter

# Calculate the number of removed trips
row_removed <- before_date_filter - after_date_filter
row_removed

data <- data_filtered
```

We removed `r row_removed` trips from our dataset.

We also added a column containing the day of the week as well as a column to indicate if the day belongs to the week-end. Official holidays are ignored for this analysis.

```{r weekday}
# Add a column for the day of the week
data$weekday <- weekdays(as.Date(data$started_at))
# Add a column
data$weekend <- ifelse(weekdays(as.Date(data$started_at)) %in% c("Saturday", "Sunday"), "Weekend", "Weekday")
```

### Number of stations -TO RUN-

```{r num_stations, results=FALSE}
# Count the number of stations in each city
num_stations_per_city <- data %>%
  group_by(city) %>%
  summarize(num_stations = n_distinct(start_station_id)) %>%
  pull(num_stations)

# Store the number of stations per city in a separate variable
num_stations_bergen <- num_stations_per_city[1]
num_stations_oslo <- num_stations_per_city[2]
num_stations_trondheim <- num_stations_per_city[3]
```

The number of stations for each city is summarized in the following table:

City  | Number of stations
-|-
Bergen | `r num_stations_bergen`
Oslo | `r num_stations_oslo`
Trondheim | `r num_stations_trondheim`

```{r top_chunk, results = FALSE}
# Group the data by city and station ID, and calculate the number of trips
trips_by_station <- data %>%
  group_by(city, start_station_id) %>%
  summarize(num_trips = n())

# Sort the data by city and num_trips, and keep only the top n stations for each city
top_no <- 20

top_stations <- trips_by_station %>%
  arrange(city, desc(num_trips)) %>%
  group_by(city) %>%
  top_n(top_no)

# Filter the original dataset to keep only trips from the top stations
data_top_stations <- data %>%
  semi_join(top_stations, by = c("city", "start_station_id"))

before_top <- nrow(data)
after_top <- nrow(data_top_stations)
removed_top <- before_top - after_top

data <- data_top_stations
```

To narrow our analysis furthermore, we will only consider the data coming from the top `r `top_no` stations in each city.

We removed `removed_top` trips from our dataset, which then contained `r after_top` trips.


### Time of rental (MAYBE NOT USED???) - NOT EVALUATED -

We reformat the columns containing the start and end time of the rental.

```{r time_col, eval=FALSE}
data <- data %>% mutate(start_time = as.ITime(started_at)) %>%
  mutate(end_time = as.ITime(ended_at))
```


### Missing weather data -TO RUN-

Given the large number of observations still available, we decided to not take into account trips with missing temperature values.

```{r missing_temp, results=FALSE}

data_filtered <- data %>% filter(!is.na(temp))

# Calculate the number of removed trips
removed_temp <- nrow(data) - nrow(data_filtered)
removed_temp

data <- data_filtered
```

We removed `r removed_temp` trips with missing temperature data.

After verification on the metesostat.net website, it seems that missing precipitations data corresponds to a value of 0.
We modified our dataset accordingly.

```{r precipitation}
data$prcp <- ifelse(is.na(data$prcp), 0, data$prcp)
```

## Data exploration

```{r trips_per_city_histo}
data %>% 
  ggplot(mapping = aes(x = city, fill = city)) + 
    geom_bar() +
  geom_text(aes(label = after_stat(count)), stat = 'count', nudge_y = 0.5,
    colour = 'black',size = 3) +
    labs(title = "Number of trips per city")
```


```{r trips_per_month}
data %>% 
  group_by(city, year_month = paste(format(started_at, "%Y-%m"), "-01", sep="")) %>% 
  summarise(n = n()) %>% 
  ggplot(mapping = aes(y = n, x = as.Date(year_month), group = city, colour = city)) + 
    geom_line() +
    geom_point() +
    labs(title = "Sum of trips per city and month", y = "number of trips", x = "")
```

## Time of the trips (NOT WORKING ELISE)

There are no missing values concerning the start or the end time of the trips.

Some background information: __may or may not be included in the report at the end__
In Bergen, bikes are available for rental throughout the year between 5am and midnight. ^[https://bergenbysykkel.no/en/faq]
The bikes in the city of Oslo are available anytime of the year between 5am and 1am. ^[https://oslobysykkel.no/en/faq]
In Trondheim, the bikes are available as long as there is no ice on the ground, 
from April to the start of December. ^[https://trondheimbysykkel.no/en/faq]

__I (Elise) want to check that this info fits our data.__

```{r time_trips, eval=FALSE}

# Bergen
# There should be no data between midnight and 5am.
bergen <- data %>% filter(city == "Bergen")
bergen %>% ggplot(aes(x = start)) +
  geom_histogram() + 
  scale_x_time(breaks = "1 hour")

# Oslo
# There should be no data between 1am and 5am.
oslo <- data %>% filter(city == "Oslo")

# Trondheim
# There should be no data for the months January to March. and some data available during the night.
trondheim <- data %>% filter(city == "Trondheim")
```



## Number of trips per price

```{r price}
ggplot(data, aes(x = price, fill = city)) +
  geom_bar() +
  labs(x = "Price (NOK)", y = "Number of Trips", title = "Number of Trips by Price") +
  scale_fill_discrete(name = "City") +
  theme_minimal()

free_trips <- round(sum(data$duration <= 3600) / nrow(data) * 100,1)
free_trips
```
Most of the trips last under 60 min (`r free_trips`%) and are included in the basis subscription.

# Weather

Evolution of the temperature and precipitations in Bergen, Oslo and Trondheim between April 2022 and November 2022.

## Temperature

```{r daily_temp}
# Calculate the mean temperature per city and day
daily_temp <- data %>%
  group_by(city, started_at) %>%
  summarize(mean_temp = mean(temp))

# Plot temperature by city
ggplot(daily_temp, aes(x = started_at, y = mean_temp, color = city)) +
  geom_line() +
  labs(x = "Date", y = "Mean Temperature", title = "Mean Temperature by City") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r temperature_trips}
ggplot(data, aes(x = temp, fill = city)) +
  geom_histogram(binwidth = 1, color = "black", position = "identity") +
  labs(x = "Temperature (Â°C)", y = "Number of Observations", title = "Number of Observations per Temperature")
```

## Precipitations

```{r precipitations}
# Calculate the mean precipitation per hour and city
hourly_prcp <- data %>%
  group_by(city, hour = format(started_at, "%H"), date = as.Date(started_at)) %>%
  summarize(mean_prcp = mean(prcp))

# Calculate the sum precipitation per day and city
daily_prcp <- hourly_prcp %>%
  group_by(city, date) %>%
  summarize(sum_prcp = sum(mean_prcp))

# Plot precipitation by city
ggplot(daily_prcp, aes(x = date, y = sum_prcp, color = city)) +
  geom_line() +
  labs(x = "Date", y = "Total Precipitation", title = "Total Precipitation per Day by City") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r precipitations_trips, message = FALSE, eval=FALSE}
 
ggplot(data, aes(x = prcp, fill = city)) +
  geom_histogram(binwidth = 1, color = "black", position = "identity") +
  labs(x = "Precipitations (mm)", y = "Number of Observations", title = "Number of Observations per Amount of precipitations")
```


# Linear Model: trip duration (Elise)


```{r model_lm}
# Fit a linear regression model with duration as the response variable and price, weekend and temperature and precipitations as predictor variables
model_lm <- lm(duration ~ price + temp + prcp + poly(temp, degree=2) + poly(prcp, degree=2) + weekend, data = data)

# Summarize the results of the model
summary(model_lm)
```

All chosen predictor variables have a significant impact on the duration of the trips.
This model fits our data quite well. The R^2 is of `r summary(model_lm)$r.squared`.


```{r model_lm}
# Create a data frame with the observed and predicted values
pred_df <- data.frame(duration = data$duration,
                      predicted = predict(model_lm))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_df, aes(x = predicted, y = duration)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
  labs(x = "Predicted duration", y = "Observed duration")
```

# Linear Model POOP
As `r free_trips`% of the trips last less than 60 min, we will focus on this data and ignore the trips lasting over an hour.

```{r lm_data}
lm_data <- data %>% filter(price == 0)
```

## Is the duration of the trip impacted by the weather?

We start with a linear regression to explore the relationship between the duration of the trip
and the temperature and precipitation.

We first plot the duration versus temperature and precipitation with a linear regression line.

```{r simple_lm_plot, fig.show='hold', out.width="49%"}

# Create the first plot: temperature vs. duration
plot1 <- ggplot(lm_data, aes(x = temp, y = duration)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Temperature", y = "Duration")

# Create the second plot: precipitation vs. duration
plot2 <- ggplot(lm_data, aes(x = prcp, y = duration)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Precipitation", y = "Duration")

# Display the two plots side-by-side
gridExtra::grid.arrange(plot1, plot2, ncol = 2)
```
The relationship of the duration of the trips with the temperature or the precipitations does not seem to be linear.
We combine these 2 predictors (temperature and precipitations) in one model.

```{r simple_lm}
# Fit a simple linear regression model with duration as the response variable and temperature and precipitation as predictor variables
simple_lm <- lm(duration ~ temp + prcp, data = lm_data)

# Summarize the results of the model
summary(simple_lm)
```

The temperature seems to have a significant positive correlation with the duration of trips and rain seems to correlate with a reduced duration of trips. While the model is significant, it does not fit the dataset optimally: the R^2 is only of `r summary(simple_lm)$r.squared`.

When including the interaction between the temperature and precipitations, the model looks as follow:

```{r simple_lm_interaction_p1}
# Fit a simple linear regression model with duration as the response variable and temperature and precipitation as predictor variables, including their interaction
simple_lm_interaction <- lm(duration ~ temp * prcp, data = lm_data)

# Summarize the results of the model
summary(simple_lm_interaction)
```
The interaction between the temperature and precipitations shows a slight significance.

```{r simple_lm_prediction}
# Create a data frame with the observed and predicted values
pred_df <- data.frame(duration = lm_data$duration,
                      predicted = predict(simple_lm_interaction))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_df, aes(x = predicted, y = duration)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
  labs(x = "Predicted duration", y = "Observed duration")
```

We might be getting a better model by using non-linear relationships such as polynomials.

```{r quad_lm}
# Fit a quadratic model
quad_lm <- lm(duration ~ temp + prcp + poly(temp, degree=2) + poly(prcp, degree=2), data = lm_data)
# Print the model summary
summary(quad_lm)

# F-test between quadratic model and simple linear model
anova(simple_lm, quad_lm)
```

```{r  plot_quad_lm}
# Create a data frame with the observed and predicted values
pred_quad_df <- data.frame(duration = lm_data$duration,
                      predicted = predict(quad_lm))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_quad_df, aes(x = predicted, y = duration)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
  labs(x = "Predicted duration", y = "Observed duration")
```

I just want to see what type of relationships are present and let r plot the smooth line without indicating a method.

```{r lm_plots, fig.show='hold', out.width="49%"}
# Create the first plot: temperature vs. duration
plot1 <- ggplot(lm_data, aes(x = temp, y = duration/60)) +
  geom_point() +
  geom_smooth() +
  labs(x = "Temperature", y = "Duration") +
  coord_cartesian(ylim = c(0, 60))

# Create the second plot: precipitation vs. duration
plot2 <- ggplot(lm_data, aes(x = prcp, y = duration/60)) +
  geom_point() +
  geom_smooth() +
  labs(x = "Precipitation", y = "Duration") +
  coord_cartesian(ylim = c(0, 60))

# Display the two plots side-by-side
gridExtra::grid.arrange(plot1, plot2, ncol = 2)
```

## Effect of the week-end on the trip duration

We want to evaluate if the trips during the week-end are longer than the trips during the week. We visualized this effect using a box-plot zooming on trips duration under 1 hour.

```{r duration_week-end_plots}
# Create a box plot of trip duration by weekday or week-end
ggplot(lm_data, aes(x = weekend, y = duration/60)) +
  geom_boxplot() +
  labs(x= "", y = "Trip Duration (minutes)") 
```

Let's integrate it in our linear model:
```{r quad_lm_weekend}
# Fit a quadratic model
quad_lm_weekend <- lm(duration ~ temp + prcp + poly(temp, degree=2) + poly(prcp, degree=2) + weekend, data = lm_data)
# Print the model summary
summary(quad_lm_weekend)

# F-test between quadratic model and simple linear model
anova(quad_lm, quad_lm_weekend)
```
The R^2 value is slightly better than before.

```{r  plot_quad_lm_weekend}
# Create a data frame with the observed and predicted values
pred_quad_weekend_df <- data.frame(duration = lm_data$duration,
                      predicted = predict(quad_lm_weekend))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_quad_weekend_df, aes(x = predicted, y = duration/60)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
  labs(x = "Predicted duration", y = "Observed duration")
```

## Including the price of trips

```{r quad_lm_weekend_price}
# Fit a quadratic model
quad_lm_weekend_price <- lm(duration ~ temp + prcp + poly(temp, degree=2) + poly(prcp, degree=2) + weekend + price, data = data)
# Print the model summary
summary(quad_lm_weekend_price)

# F-test between quadratic model and simple linear model
anova(quad_lm, quad_lm_weekend_price)
```
The R^2 value increases dramatically!

```{r  plot_quad_lm_weekend_price}
# Create a data frame with the observed and predicted values
pred_quad_weekend_price_df <- data.frame(duration = data$duration,
                      predicted = predict(quad_lm_weekend_price))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_quad_weekend_price_df, aes(x = predicted, y = duration/60)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
  labs(x = "Predicted duration", y = "Observed duration")
```

just try

```{r}

data_subset <- sample_n(data, 100000)
model <- lm(duration ~ temp, data = data_subset %>% filter(duration < 0.5 * 60 * 60))

summary(model)

```
```{r}
data_subset %>%
  filter(duration < 0.5 * 60 * 60) %>%
  ggplot(mapping = aes(y = duration, x = temp)) +
    geom_point()
```
```{r}
data_subset %>%
  filter(duration < 0.5 * 60 * 60)%>%
  ggplot(mapping = aes(y = duration, x = factor(coco))) +
    geom_boxplot()
```


# Generalised Linear Model with family set to Poisson (Pascal)

predict how many bikes we need to have available at given station. 

First we prepare the data.

```{r data_GLM_Poisson}

# Group the data by city and station ID, and calculate the number of trips
trips_by_station <- data %>%
  group_by(city, start_station_id) %>%
  summarise(num_trips = n(), 
            city = last(city)) %>% 
  arrange(desc(num_trips)) 

# show stations with most trips
kable(head(trips_by_station, 10))

head(trips_by_station)
```


Outgoing trips per station

```{r}

# limit to top 10 stations
top_station_ids <- trips_by_station$start_station_id[1:6] 
summer_vacation_int <- interval(ymd("2022-06-18"), ymd("2022-08-21"))

trips_per_station <- data %>% 
  filter(start_station_id %in% top_station_ids) %>% 
  mutate(started_trip_date = as.Date(started_at)) %>% 
  group_by(start_station_id, started_trip_date) %>% 
  summarise(
      trips = n(),
      avg_temp = mean(temp, na.rm = T), # beware: this is not average temperature of the day, but avg temperature of trips
      max_temp = max(temp, na.rm = T),
      min_temp = min(temp, na.rm = T),
      avg_prcp = mean(prcp, na.rm = T)
    ) %>% 
  mutate(
      month = month(started_trip_date),
      weekday = lubridate::wday(started_trip_date, week_start = 1, label = T),
      weekend = lubridate::wday(started_trip_date, week_start = 1) >= 5,
      summer_vacation = started_trip_date %within% summer_vacation_int
    )
```

Plot correlation

```{r}
trips_per_station %>%
  ggplot(mapping = aes(y = trips, x = avg_temp)) + 
  facet_wrap(~start_station_id) + 
  geom_point() +
  stat_smooth(method = "glm", method.args = list(family = 'poisson'), colour = '#F8766D') +
  labs(title = "Number of trips vs. average temperature", 
       subtitle = 'At 6 most frequented stations',
       y = "number of trips", 
       x = "avg. temperature CÂ°")
```

Make model

```{r}

trips_per_temp_glm <- glm(trips ~ avg_temp + factor(start_station_id)-1,
                     family = "poisson",
                     data = trips_per_station)
summary(trips_per_temp_glm)
```

```{r}
glm_temp_coef <- exp(coef(trips_per_temp_glm)['avg_temp'])

sprintf('For every increase in one temperature we get a %.2f times increase in trips a day.', glm_temp_coef)

```

# Generalised Linear Model with family set to Binomial

# Generalised Additive Model (Emilia)
Do weather conditions affect the duration of trips?

```{r gam_dataset}

#retrieve only data about the duration and weather parameters 
weather <- data %>% select(c(city, duration, temp, dwpt, prcp, snow, wpgt,wdir,wspd))
colnames(weather) <- c("city","Duration", "Temperature","Dew_Point", "Precipitation", "Snow_Depth", "Wind_Peak_Gust", "Wind_direction", "Wind_spead")
head(weather)

#remove NA values and chose relevant weather parameters
weather<- na.omit(weather[, c("city","Duration","Temperature", "Dew_Point", "Precipitation", "Wind_direction","Wind_spead")])
str(weather)

#display relationship of duration with the weather variables
template.graph.weather <- ggplot(data = weather,
                                 mapping = aes(y = log(Duration))) +
  geom_point() +
  geom_smooth()

template.graph.weather + aes(x = Temperature)
template.graph.weather + aes(x = Dew_Point)
template.graph.weather + aes(x = Precipitation)
template.graph.weather + aes(x = Wind_direction)
template.graph.weather + aes(x = Wind_spead)
```

```{r gam}
gam <- gam(log(Duration) ~ s(Temperature) + s(Dew_Point) + s(Precipitation) + s(Wind_direction) +s(Wind_spead),
              data = weather)
summary(gam)

plot(gam, residuals = TRUE, pages = 1, shade = TRUE)
```

The GAM model states that temperature, dew_point, wind direction and wind speed have a significant effect on the duration of a bike trip. Whereas, the precipitation doesn't seem to have significant impact.

# Neural Network

# Support Vector Machine

We predict the city based on latitude and longitude.

```{r cities_dataset}
# prepare data
cities_start <- data %>% select(c(start_station_latitude, start_station_longitude, city))
colnames(cities_start) <- c("latitude", "longitude","city")

cities_end <- data %>% select(c(end_station_latitude, end_station_longitude, city))
colnames(cities_end) <- c("latitude", "longitude","city")

cities <- rbind(cities_start, cities_end)
rm(cities_start, cities_end)

cities$city <- as.factor(cities$city)

print(colSums(is.na(cities))) # no missing values!

set.seed(10)
cities_subset <- rbind(
  cities[city == 'Trondheim'][sample(nrow(cities[city == 'Trondheim']), 33300), ],
  cities[city == 'Oslo'][sample(nrow(cities[city == 'Oslo']), 33300), ],
  cities[city == 'Bergen'][sample(nrow(cities[city == 'Bergen']), 33300), ]
)
cities_subset %>% 
  group_by(city) %>% 
  summarise('number' = n())
  
```

```{r}
cities_subset %>% 
  ggplot(mapping = aes(x = longitude, y = latitude, color = city)) +
    geom_point() +
    labs(title = "Start/End points per city")
```


```{r cities_dataset_split}
# form training and testing datasets
set.seed(10)
indices <- createDataPartition(cities_subset$city, p=0.7, list=F)

train_cities <- cities_subset %>% slice(indices)
test_cities <-  cities_subset %>% slice(-indices)
test_cities_truth <- cities_subset %>% slice(-indices) %>% pull(city)
```

## Linear kernel

```{r SVM_city_linear}
cities_svm <- svm(city ~. , train_cities, kernel = "linear", scale = TRUE, cost = 10)

#make predictions
test_pred <- predict(cities_svm, test_cities)
table(test_pred)
```

```{r}
plot(cities_svm, train_cities, latitude ~ longitude)
```

```{r}
#evaluate the results
conf_matrix <- confusionMatrix(test_pred, test_cities_truth)
conf_matrix

```

Only 3 data points were misclassified. We built a SVM with a prediction accuracy of nearly 100%.
__cf. Iris lab: We should do some cross-validation to ensure that we are not simply lucky with the train/test split. But on the other end... it is good enough, no?__

## Radial kernel

```{r SVM_city_radial}
cities_svm_radial <- svm(city ~. , train_cities, kernel = "radial", scale = TRUE, cost = 100)

# plot classification
plot(cities_svm_radial, train_cities, latitude ~ longitude)
```

```{r}
#make predictions
test_pred2 <- predict(cities_svm_radial, test_cities)
table(test_pred2)
```

```{r}
#evaluate the results
conf_matrix2 <- confusionMatrix(test_pred2, test_cities_truth)
conf_matrix2
```
Same results with another type of kernel.

__We ignore the 3 wrong values (at least for now :)__ 

# Solve an optimisation problem
