---
title: "Analysis of bike sharing data"
author: "Elise Gourri, Emilia Marlene Ribeiro Peixoto, Pascal Albisser"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: TRUE
    code_folding: hide
editor_options:
  chunk_output_type: console
---


```{r libraries, message = FALSE, echo = FALSE}
# data loading
library(readr)
library(glue)
library(RCurl)

# data manipulation and visualization
library(data.table) # aggregation of large data
library(tidyverse)
library(lubridate)
library(kableExtra) # to render tables in html
library(gridExtra)
library(ggplot2)
library(dplyr)
library(scales)

# machine learning libraries
library(e1071)
library(caret)
library(kernlab)
library(mgcv)

# geo-packages
library("geosphere")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = function() {
  if (knitr::is_latex_output())
    "latex" else "pipe"
})

options(scipen = 999)
```

# Dataset

![Locations of the bike sharing systems](data/map_norway.png)

This report focuses on the study of bike-sharing systems in three cities in Norway: [Bergen](https://bergenbysykkel.no/en/open-data/realtime), [Oslo](https://oslobysykkel.no/en/open-data/realtime), and [Trondheim](https://trondheimbysykkel.no/en/open-data/realtime). The analysis is based on freely available data from the year 2022, encompassing trip details and station information. We also included [weather data](https://meteostat.net/en/).  
The bike-sharing systems operate on a docked bike model, with bikes available at official stations and returned to other official stations.  
The inclusion of weather data allows for a comprehensive understanding of factors influencing bike usage. The study aims to demonstrate the application of machine learning techniques to improve bike-sharing systems' efficiency and planning.

Our dataset is composed of the following variables:

| Field Name | Data Type | Description |
| --- | --- | --- |
| started_at | DATETIME | Start of trip |
| ended_at | DATETIME | End of the trip |
| duration | INTEGER | Duration of trip in seconds |
| start_station_id | INTEGER | Identifier of start station |
| start_station_latitude | DECIMAL | Location latitude of start station |
| start_station_longitude | DECIMAL | Location longitude of start station |
| end_station_id | INTEGER | Identifier of end station |
| end_station_latitude | DECIMAL | Location latitude of end station |
| end_station_longitude | DECIMAL | Location longitude of end station |
| city | STRING | Name of city |
| temp | DECIMAL | Approximate temperature during the trip |
| dwpt | DECIMAL | Dew point in Â°C |
| prcp | DECIMAL | Total Precipitation in mm |
| snow | INTEGER | Snow Depth in mm |
| wpgt | DECIMAL | Wind Peak Gust in km/h |
| tsun | BOOLEAN | Total sunshine duration in minutes |
| coco | INTEGER | Weather Condition Code from 1 (clear) to 27 (storm) |

__add the following if used__
WDIR	Wind (From) Direction
WSPD	Average Wind Speed
RHUM	Relative Humidity
PRES	Sea-Level Air Pressure

More details concerning the labelling of the weather data is available on the [Meteostat website](https://dev.meteostat.net/formats.html#time-format).

```{r download_dataset, eval=FALSE}

years <- 2022
months <- sprintf("%02d", 01:12)

# download trips data in the 3 cities
city_links <- list(Trondheim="https://data.urbansharing.com/trondheimbysykkel.no/trips/v1/{year}/{month}.csv", 
                   Oslo="https://data.urbansharing.com/oslobysykkel.no/trips/v1/{year}/{month}.csv", 
                   Bergen="https://data.urbansharing.com/bergenbysykkel.no/trips/v1/{year}/{month}.csv")

trip_data = data.frame()

for (city in names(city_links)) {
  link <- city_links[[city]]
  for(year in years){
    for(month in months){
      current_link <- glue(link)
      if(url.exists(current_link)) {
        message('downloading: ', current_link)
        data <- read.csv(current_link)
        if(nrow(data) > 0) {
          data$city <- city
        }
        trip_data <- rbind(trip_data, data)
      }
    }
  }
}

# download weather data for the 3 cities
weather_station_ids <- list(Trondheim="01257", 
                            Oslo="01492", 
                            Bergen="01317")

weather_data = data.frame()

for (city in names(weather_station_ids)) {
  id <- weather_station_ids[[city]]
  link <- glue("https://bulk.meteostat.net/v2/hourly/{id}.csv.gz")
  if(url.exists(link)) {
    message('downloading: ', link)
    data <- read_csv(link, col_names = FALSE)
    data$city <- city
    if(nrow(data) > 0) {
      data$city <- city
    }
    weather_data <- rbind(weather_data, data)
  }
}

colnames(weather_data) <- c("date", "hour", "temp", "dwpt", "rhum", "prcp", "snow", "wdir", "wspd", "wpgt", "pres", "tsun", "coco", "city")

# merging the files

## prepare
trip_data$started_at <- as.POSIXct(trip_data$started_at)
trip_data$ended_at <- as.POSIXct(trip_data$ended_at)
trip_data$started_at_date <- as.Date(trip_data$started_at)
trip_data$started_at_hour <- format(trip_data$started_at, "%H")

## test the join
left_join(trip_data[1:100,], weather_data, by = join_by(x$started_at_date == y$date, x$started_at_hour == y$hour, city), keep = TRUE) %>% 
  select(started_at, city.x, city.y, date, hour)

## join!
trips <- left_join(trip_data, weather_data, by = join_by(x$started_at_date == y$date, x$started_at_hour == y$hour, city))

## tidy up
trips <- trips %>% 
  select(
    -started_at_hour,
    -started_at_date
  )

# write to file (complete)
write.csv(trips, "data/trips.csv", row.names = FALSE)

# subset for smaller file size
trips_col_subset <- trips %>% 
  select(
    -start_station_name,
    -start_station_description,
    -end_station_description,
    -end_station_name,
    -rhum,
    -wdir,
    -wspd,
    -pres
  )

write.csv(trips_col_subset, "data/trips_s.csv", row.names = FALSE)
```

```{r load_data, cache=TRUE}
data_raw <- fread("data/trips_2022.csv")
kable(head(data_raw))
```

## Data preparation

We needed to modify the dataset to be able to perform our analysis.

### Duration of trips

```{r duration_nb, results=FALSE}
data <- data_raw

# count the number of trips that lasted 2 minutes or less and starting and ending at the same station
short_trips <- nrow(subset(data, duration <= 120 & start_station_id == end_station_id))
short_trips

# count the number of trips that lasted 24 hours and more
long_trips <- nrow(subset(data, duration >= 86400))
long_trips

# remove the data concerning the "short" and long "trips"
data <- data[!(data$duration <= 120 & data$start_station_id == data$end_station_id) & !(data$duration >= 86400), ]

# longest trip (under 24 hours)
max(data$duration)
```

The duration of each trips is indicated in seconds.

We removed `r short_trips` trips which lasted 2 min or less and with the same start and end stations. They probably correspond to users picking up a bike and immediately returning it. None of the trips lasted more than 24 hours.

__box plot is not the best... we need to do it better__
```{r duration_plot, eval=FALSE}
data %>%
  ggplot(mapping = aes(x = city, y = duration)) +
    geom_boxplot() +
    labs(title = "Distribution of the trips duration")
```
Most of the trips were rather short trips. Some trips lasted several hours, the longest being approximately 19 hours Oslo.

### Price of the trips

We added a column containing the price per trip.
In the 3 cities Bergen, Oslo and Trondheim, the first 60 min are free, each 15 supplementary minutes cost NOK 15 ^[https://trondheimbysykkel.no/en/how-it-works].

```{r price_trips}
data$price <- ifelse(data$duration <= 3600, 0, (ceiling((data$duration - 3600) / 900)) * 15)
```

### Add haversine distance

```{r}
data <- data %>% 
  mutate(h_distance = distHaversine(
    cbind(start_station_longitude, start_station_latitude),
    cbind(end_station_longitude, end_station_latitude))) 

kable(head(data %>% 
  select(start_station_longitude, start_station_latitude, end_station_longitude, end_station_latitude, h_distance, duration)))
```


### Retain only data from April to November -TO RUN-

In the cities of Bergen^[https://bergenbysykkel.no/en/faq] and Oslo^[https://oslobysykkel.no/en/how-it-works], bikes are available all year round.
We therefore focused our analysis on the months between April and November.
In Trondheim, the bikes are available as long as there is no ice on the ground,
from April to the start of December^[https://trondheimbysykkel.no/en/faq].  

```{r filter_dates, results=FALSE}
# Create a new column with the month of each trip start date
data <- data %>%
  mutate(start_month = month(started_at))

# Count the number of rows in the original dataframe
before_date_filter <- nrow(data)

# Filter the trips for the months between April and November
data_filtered <- data %>%
  filter(start_month >= 4 & start_month <= 11)

# Count the number of rows in the filtered dataframe
after_date_filter <- nrow(data_filtered)

# Calculate the number of removed trips
row_removed <- before_date_filter - after_date_filter

data <- data_filtered
```

We removed `r row_removed` trips from our dataset.

We also added a column containing the day of the week as well as a column to indicate if the day belongs to the week-end. Official holidays are ignored for this analysis.

```{r weekday}
# Add a column for the day of the week
data$weekday <- weekdays(as.Date(data$started_at))
# Add a column
data$weekend <- ifelse(weekdays(as.Date(data$started_at)) %in% c("Saturday", "Sunday"), "Weekend", "Weekday")
```

### Number of stations -TO RUN-

```{r num_stations, results=FALSE}
# Count the number of stations in each city
num_stations_per_city <- data %>%
  group_by(city) %>%
  summarize(num_stations = n_distinct(start_station_id)) %>%
  pull(num_stations)

# Store the number of stations per city in a separate variable
num_stations_bergen <- num_stations_per_city[1]
num_stations_oslo <- num_stations_per_city[2]
num_stations_trondheim <- num_stations_per_city[3]
```

The number of stations for each city is summarized in the following table:

City  | Number of stations
-|-
Bergen | `r num_stations_bergen`
Oslo | `r num_stations_oslo`
Trondheim | `r num_stations_trondheim`

```{r top_chunk, results = FALSE}
# Group the data by city and station ID, and calculate the number of trips
trips_by_station <- data %>%
  group_by(city, start_station_id) %>%
  summarize(num_trips = n())

# Sort the data by city and num_trips, and keep only the top n stations for each city
top_no <- 20

top_stations <- trips_by_station %>%
  arrange(city, desc(num_trips)) %>%
  group_by(city) %>%
  top_n(top_no)

# Filter the original dataset to keep only trips from the top stations
data_top_stations <- data %>%
  semi_join(top_stations, by = c("city", "start_station_id"))

before_top <- nrow(data)
after_top <- nrow(data_top_stations)
removed_top <- before_top - after_top

data <- data_top_stations
```

To narrow our analysis furthermore, we only considered the data coming from the top `r `top_no` stations in each city.

We removed `removed_top` trips from our dataset, which then contained `r after_top` trips.


### Time of rental - TO RUN -

We reformated the columns containing the start and end time of the bike rentals.

```{r time_col}
data <- data %>% mutate(start_time = as.ITime(started_at)) %>%
  mutate(end_time = as.ITime(ended_at))
```


### Missing weather data -TO RUN-

Given the large number of observations still available, we decided to not take into account trips with missing temperature values.

```{r missing_temp, results=FALSE}
data_filtered <- data %>% filter(!is.na(temp))

# Calculate the number of removed trips
removed_temp <- nrow(data) - nrow(data_filtered)

data <- data_filtered
```

We removed `r removed_temp` trips with missing temperature data.

After verification on the [meteostat.net website] (https://meteostat.net/en/), it seems that missing precipitations data corresponds to a value of 0.
We modified our dataset accordingly.

```{r precipitation}
data$prcp <- ifelse(is.na(data$prcp), 0, data$prcp)
```

## Data exploration

```{r trips_per_city_histo}
data %>% 
  ggplot(mapping = aes(x = city, fill = city)) + 
    geom_bar() +
  geom_text(aes(label = after_stat(count)), stat = 'count', nudge_y = 0.5,
    colour = 'black',size = 3) +
    labs(title = "Number of Trips per City")
```

```{r city_percent}
# Calculate the percentage of observations for each city
city_percentages <- data %>%
  group_by(city) %>%
  summarize(percentage = n() / nrow(data) * 100)

# Store the percentage for each city in separate variables
bergen_percentage <- city_percentages$percentage[city_percentages$city == "Bergen"]
trondheim_percentage <- city_percentages$percentage[city_percentages$city == "Trondheim"]
oslo_percentage <- city_percentages$percentage[city_percentages$city == "Oslo"]
```

The city with most of the trips in our dataset was Oslo (`r oslo_percentage` % of the observations), followed by Bergen (`r bergen_percentage` %). The data of the city of Trondheim represented `r trondheim_percentage` % of our dataset.

```{r trips_per_month}
data %>% 
  group_by(city, year_month = paste(format(started_at, "%Y-%m"), "-01", sep="")) %>% 
  summarise(n = n()) %>% 
  ggplot(mapping = aes(y = n, x = as.Date(year_month), group = city, colour = city)) + 
    geom_line() +
    geom_point() +
    labs(title = "Sum of Trips per City and Month", y = "Number of Trips", x = "")

# Extract the month from the "started_at" column
data$month <- month(data$started_at)

# Count the number of observations for each month
month_counts <- table(data$month)

# Find the month with the highest number of observations
max_month <- names(month_counts)[which.max(month_counts)]
```

The month with the most trips in our dataset was August.

## Time of the trips

```{r time_trips_plot, eval=FALSE}
# Create a new variable for the hour of the day
data$hour <- hour(data$started_at)

# Group the data by city and hour, and calculate the count of observations
hourly_counts <- data %>%
  group_by(city, hour) %>%
  summarize(count = n())

# Create the plot
ggplot(hourly_counts, aes(x = hour, y = count, color = city)) +
  geom_line() +
  geom_point() +
  labs(x = "Hour of the Day", y = "Number of Trips")

# Group the data by hour, and calculate the count of observations
hourly_counts2 <- data %>%
  group_by(hour) %>%
  summarize(count = n())

# Find the three hours with the maximum count
top_hours <- hourly_counts2 %>%
  slice_max(order_by = count, n = 3) %>%
  pull(hour)

# Store the three hours in separate variables
hour1 <- top_hours[1]
hour2 <- top_hours[2]
hour3 <- top_hours[3]
```

There are no missing values concerning the start or the end time of the trips.  
Our dataset contains bike trips throughout the day for almost every city. 
We could identify 2 peak times of usage: in the morning at `r #hour3`am and in the afternoon between `r #hour1`pm and `r #hour2`pm.

## Number of trips per price

```{r price}
ggplot(data, aes(x = price, fill = city)) +
  geom_bar() +
  labs(x = "Price (NOK)", y = "Number of Trips", title = "Number of Trips by Price") +
  scale_fill_discrete(name = "City") +
  theme_minimal()

free_trips <- round(sum(data$duration <= 3600) / nrow(data) * 100,1)
```
Most of the trips last under 60 min (`r free_trips`%) and are included in the basis subscription.

# Weather

Evolution of the temperature and precipitations in Bergen, Oslo and Trondheim between April 2022 and November 2022.

## Temperature

```{r daily_temp}
# Calculate the mean temperature per city and day
daily_temp <- data %>%
  group_by(city, started_at) %>%
  summarize(mean_temp = mean(temp))

# Plot temperature by city
ggplot(daily_temp, aes(x = started_at, y = mean_temp, color = city)) +
  geom_line() +
  labs(x = "Date", y = "Mean Temperature", title = "Mean Temperature by City") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r temperature_trips}
ggplot(data, aes(x = temp, fill = city)) +
  geom_histogram(binwidth = 1, color = "black", position = "identity") +
  labs(x = "Temperature (Â°C)", y = "Number of Observations", title = "Number of Observations per Temperature")
```

## Precipitations

```{r precipitations}
# Calculate the mean precipitation per hour and city
hourly_prcp <- data %>%
  group_by(city, hour = format(started_at, "%H"), date = as.Date(started_at)) %>%
  summarize(mean_prcp = mean(prcp))

# Calculate the sum precipitation per day and city
daily_prcp <- hourly_prcp %>%
  group_by(city, date) %>%
  summarize(sum_prcp = sum(mean_prcp))

# Plot precipitation by city
ggplot(daily_prcp, aes(x = date, y = sum_prcp, color = city)) +
  geom_line() +
  labs(x = "Date", y = "Total Precipitation", title = "Total Precipitation per Day by City") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r precipitations_trips, message = FALSE, eval=FALSE}
 
ggplot(data, aes(x = prcp, fill = city)) +
  geom_histogram(binwidth = 1, color = "black", position = "identity") +
  labs(x = "Precipitations (mm)", y = "Number of Observations", title = "Number of Observations per Amount of precipitations")
```


# Linear Model: trip duration (Elise)

We first used a linear model using weather data (temperature and precipitation) as well as the day (weekday or weekend) as well as the time of the day to predict the duration of the trips.

```{r model_lm}
# Fit a linear regression model with duration as the response variable and price, weekend, hour and temperature and precipitations as predictor variables
# model_lm <- lm(duration ~ price + temp + prcp + poly(temp, degree=2) + poly(prcp, degree=2) + weekend + hour + poly(hour, degree=2), data = data)

# Summarize the results of the model
# summary(model_lm)
```

All chosen predictor variables have a significant impact on the duration of the trips.
This model fits our data quite well. The R^2 is of `r #summary(model_lm)$r.squared`.

```{r model_lm_}
# Create a data frame with the observed and predicted values
# pred_df <- data.frame(duration = data$duration,
#                       predicted = predict(model_lm))

# Create a scatter plot of observed vs. predicted values
# ggplot(pred_df, aes(x = predicted, y = duration)) +
#   geom_point() +
#   geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
#   labs(x = "Predicted Duration", y = "Observed Duration")
```

# Generalised Additive Model (Emilia)

Another approach to predict the duration of the trips was to use a Generalise Additive Model (GAM) to study how the  weather conditions affect the duration of trips.

```{r gam_dataset}

#retrieve only data about the duration and weather parameters 
weather <- data %>% select(c(city, duration, temp, dwpt, prcp, snow, wpgt))#, wdir,wspd))
colnames(weather) <- c("city","Duration", "Temperature","Dew_Point", "Precipitation", "Snow_Depth", "Wind_Peak_Gust")#, "Wind_direction", "Wind_spead")
head(weather)

#remove NA values and chose relevant weather parameters
weather<- na.omit(weather[, c("city","Duration","Temperature", "Dew_Point", "Precipitation")])#, "Wind_direction","Wind_spead")])
#str(weather)

#display relationship of duration with the weather variables
template.graph.weather <- ggplot(data = weather,
                                 mapping = aes(y = log(Duration))) +
  geom_point() +
  geom_smooth()

template.graph.weather + aes(x = Temperature)
template.graph.weather + aes(x = Dew_Point)
template.graph.weather + aes(x = Precipitation)
#template.graph.weather + aes(x = Wind_direction)
#template.graph.weather + aes(x = Wind_spead)
```

```{r gam}
gam <- gam(log(Duration) ~ s(Temperature) + s(Dew_Point) + s(Precipitation), # + s(Wind_direction) +s(Wind_spead),
              data = weather)
summary(gam)

plot(gam, residuals = TRUE, pages = 1, shade = TRUE)
```

The GAM model states that temperature, dew_point, wind direction and wind speed have a significant effect on the duration of a bike trip. Whereas, the precipitation doesn't seem to have a significant impact.

__Conclusions, also with regard to linear model??__

# Generalised Linear Model with family set to Poisson (Pascal)

predict how many bikes we need to have available at given station. 

First we prepare the data.

```{r data_GLM_Poisson}

# Group the data by city and station ID, and calculate the number of trips
trips_by_station <- data_raw %>%
  group_by(city, start_station_id) %>%
  summarise(num_trips = n(), 
            city = last(city)) %>% 
  arrange(desc(num_trips)) 

# show stations with most trips
kable(head(trips_by_station, 10))

head(trips_by_station)
```


Outgoing trips per station

```{r}

# limit to top 10 stations
top_station_ids <- trips_by_station$start_station_id[1:6] 
summer_vacation_int <- interval(ymd("2022-06-18"), ymd("2022-08-21"))

trips_per_station <- data %>% 
  filter(start_station_id %in% top_station_ids) %>% 
  mutate(started_trip_date = as.Date(started_at)) %>% 
  group_by(start_station_id, started_trip_date) %>% 
  summarise(
      trips = n(),
      avg_temp = mean(temp, na.rm = T), # beware: this is not average temperature of the day, but avg temperature of trips
      max_temp = max(temp, na.rm = T),
      min_temp = min(temp, na.rm = T),
      avg_prcp = mean(prcp, na.rm = T)
    ) %>% 
  mutate(
      month = month(started_trip_date),
      weekday = lubridate::wday(started_trip_date, week_start = 1, label = T),
      weekend = lubridate::wday(started_trip_date, week_start = 1) >= 5,
      summer_vacation = started_trip_date %within% summer_vacation_int
    )
```

Plot correlation

```{r}
trips_per_station %>%
  ggplot(mapping = aes(y = trips, x = avg_temp)) + 
  facet_wrap(~start_station_id) + 
  geom_point() +
  stat_smooth(method = "glm", method.args = list(family = 'poisson'), colour = '#F8766D') +
  labs(title = "Number of trips vs. average temperature", 
       subtitle = 'At 6 most frequented stations',
       y = "number of trips", 
       x = "avg. temperature CÂ°")
```

Make model

```{r}

trips_per_temp_glm <- glm(trips ~ avg_temp + factor(start_station_id)-1,
                     family = "poisson",
                     data = trips_per_station)
summary(trips_per_temp_glm)
```

```{r}
glm_temp_coef <- exp(coef(trips_per_temp_glm)['avg_temp'])

sprintf('For every increase in one temperature we get a %.2f times increase in trips a day.', glm_temp_coef)

```

# Generalised Linear Model with family set to Binomial

# Neural Network

# Support Vector Machine

We tested if a Support Vector Machine could help predict the city based on the latitude and longitude of the stations.
This could for example be useful to adopt a nation-wide bike-sharing system without relying on station ID to identify the city of the station.

```{r cities_dataset}
# prepare data
cities_start <- data %>% select(c(start_station_latitude, start_station_longitude, city))
colnames(cities_start) <- c("latitude", "longitude","city")

cities_end <- data %>% select(c(end_station_latitude, end_station_longitude, city))
colnames(cities_end) <- c("latitude", "longitude","city")

cities <- rbind(cities_start, cities_end)
rm(cities_start, cities_end)

cities$city <- as.factor(cities$city)

print(colSums(is.na(cities))) # no missing values!

set.seed(10)
cities_subset <- rbind(
  cities[city == 'Trondheim'][sample(nrow(cities[city == 'Trondheim']), 33300), ],
  cities[city == 'Oslo'][sample(nrow(cities[city == 'Oslo']), 33300), ],
  cities[city == 'Bergen'][sample(nrow(cities[city == 'Bergen']), 33300), ]
)
cities_subset %>% 
  group_by(city) %>% 
  summarise('number' = n())
```

```{r svm_plot}
cities_subset %>% 
  ggplot(mapping = aes(x = longitude, y = latitude, color = city)) +
    geom_point() +
    labs(title = "Start/End points per city")
```


```{r cities_dataset_split}
# form training and testing datasets
set.seed(10)
indices <- createDataPartition(cities_subset$city, p=0.7, list=F)

train_cities <- cities_subset %>% slice(indices)
test_cities <-  cities_subset %>% slice(-indices)
test_cities_truth <- cities_subset %>% slice(-indices) %>% pull(city)
```

## Linear kernel

```{r SVM_city_linear}
cities_svm <- svm(city ~. , train_cities, kernel = "linear", scale = TRUE, cost = 10)

#make predictions
test_pred <- predict(cities_svm, test_cities)
table(test_pred)
```

```{r}
plot(cities_svm, train_cities, latitude ~ longitude)
```

```{r}
#evaluate the results
conf_matrix <- confusionMatrix(test_pred, test_cities_truth)
conf_matrix
```

Only 3 data points were misclassified. We built a SVM with a prediction accuracy of nearly 100%.
__is it still the case?__

## Radial kernel

```{r SVM_city_radial}
cities_svm_radial <- svm(city ~. , train_cities, kernel = "radial", scale = TRUE, cost = 100)

# plot classification
plot(cities_svm_radial, train_cities, latitude ~ longitude)
```

```{r}
#make predictions
test_pred2 <- predict(cities_svm_radial, test_cities)
table(test_pred2)
```

```{r}
#evaluate the results
conf_matrix2 <- confusionMatrix(test_pred2, test_cities_truth)
conf_matrix2
```
Same results with another type of kernel.

__We ignore the 3 wrong values. choose only one type of kernel???__ 

# Solve an optimisation problem
