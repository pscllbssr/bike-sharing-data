---
title: "Analysis of norwegian bike sharing data"
author: "Elise Gourri, Emilia Marlene Ribeiro Peixoto, Pascal Albisser"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: TRUE
    code_folding: hide
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
options(knitr.table.format = function() {
  if (knitr::is_latex_output())
    "latex" else "pipe"
})

options(scipen = 999)
```

```{r libraries, message = FALSE, echo = FALSE}
# data loading
library(readr)
library(glue)
library(RCurl)

# data manipulation and visualization
library(data.table) # aggregation of large data
library(tidyverse)
library(lubridate)
library(kableExtra) # to render tables in html
library(gridExtra)
library(ggplot2)
library(dplyr)
library(scales)

# machine learning libraries
library(e1071)
library(caret)
library(kernlab)
library(mgcv)
library(neuralnet)

# geo-packages
library("geosphere")
```


# Bike sharing in Norway

```{r picture, echo=FALSE, out.width="50%", fig.cap="Locations of the bike sharing systems", fig.align="center"}
knitr::include_graphics("data/map_norway.png")
```

This report focuses on the study of bike sharing systems in three cities in Norway: [Bergen](https://bergenbysykkel.no/en/open-data/realtime), [Oslo](https://oslobysykkel.no/en/open-data/realtime), and [Trondheim](https://trondheimbysykkel.no/en/open-data/realtime). The analysis is based on freely available data from the year 2022, encompassing trip details and station information. We also included [weather data](https://meteostat.net/en/).  
The bike sharing systems operate on a docked bike model, with bikes available at official stations and returned to other official stations.  
The inclusion of weather data allows for a comprehensive understanding of factors influencing bike usage. The study aims to demonstrate the application of machine learning techniques to improve bike-sharing systems' efficiency and planning.

Our dataset is composed of the following variables:

| Field Name | Data Type | Description |
| :--- | :--- | :--- |
| started_at | DATETIME | Start of trip |
| ended_at | DATETIME | End of the trip |
| duration | INTEGER | Duration of trip in seconds |
| start_station_id | INTEGER | Identifier of start station |
| start_station_latitude | DECIMAL | Location latitude of start station |
| start_station_longitude | DECIMAL | Location longitude of start station |
| end_station_id | INTEGER | Identifier of end station |
| end_station_latitude | DECIMAL | Location latitude of end station |
| end_station_longitude | DECIMAL | Location longitude of end station |
| city | STRING | Name of city |
| temp | DECIMAL | Approximate temperature during the trip |
| dwpt | DECIMAL | Dew point in °C |
| prcp | DECIMAL | Total Precipitation in mm |
| snow | INTEGER | Snow Depth in mm |
| wpgt | DECIMAL | Wind Peak Gust in km/h |
| tsun | BOOLEAN | Total sunshine duration in minutes |
| coco | INTEGER | Weather Condition Code from 1 (clear) to 27 (storm) |

__add the following if used__
WDIR	Wind (From) Direction
WSPD	Average Wind Speed
RHUM	Relative Humidity
PRES	Sea-Level Air Pressure

More details concerning the labelling of the weather data is available on the [Meteostat website](https://dev.meteostat.net/formats.html#time-format).

Our dataset looks as follow:

```{r load_data, cache=TRUE}
data_raw <- fread("data/trips_2022.csv")
kable(head(data_raw))
```

# Data preparation

We needed to modify the dataset to be able to perform our analysis.

## Duration of trips

```{r duration_nb, results=FALSE}
data <- data_raw

# count the number of trips that lasted 2 minutes or less and starting and ending at the same station
short_trips <- nrow(subset(data, duration <= 120 & start_station_id == end_station_id))
short_trips

# count the number of trips that lasted 24 hours and more
long_trips <- nrow(subset(data, duration >= 86400))
long_trips

# remove the data concerning the "short" and long "trips"
data <- data[!(data$duration <= 120 & data$start_station_id == data$end_station_id) & !(data$duration >= 86400), ]

# longest trip (under 24 hours)
max(data$duration)
```

We removed `r short_trips` trips which lasted 2 min or less and with the same start and end stations. They probably correspond to users picking up a bike and immediately returning it. None of the trips lasted more than 24 hours.

__box plot is not the best... we need to do it better__
```{r duration_plot, eval=TRUE}
data %>%
  ggplot(mapping = aes(x = city, y = duration)) +
    geom_boxplot() +
    labs(title = "Distribution of the trips duration")
```

Most of the trips were rather short trips. Some trips lasted several hours, the longest being approximately 19 hours Oslo.

## Price of the trips

We added a column containing the price per trip.
In the 3 cities Bergen, Oslo and Trondheim, the first 60 min are free, each 15 supplementary minutes cost NOK 15 ^[https://trondheimbysykkel.no/en/how-it-works].

```{r price_trips}
data$price <- ifelse(data$duration <= 3600, 0, (ceiling((data$duration - 3600) / 900)) * 15)
```

## Haversine distance

We added the haversine distance for each trip between the start and end stations. Of course, following a straight line on a bike trip in a city is not possible, as many buildings and other obstacles exist. But this calculation at least approximates how far the stations are apart. 

```{r haversine}
data <- data %>% 
  mutate(h_distance = distHaversine(
    cbind(start_station_longitude, start_station_latitude),
    cbind(end_station_longitude, end_station_latitude))) 

kable(head(data %>% 
  select(start_station_longitude, start_station_latitude, end_station_longitude, end_station_latitude, h_distance, duration)))
```

We can also plot the distance in relation to duration of the trips.

```{r haversine_plot}
data %>% 
  sample_n(10000) %>% 
  ggplot(mapping = aes(x = h_distance, y = duration)) +
  geom_point() +
  labs(x = 'Haversine distance (m)', y = 'Duration (s)', title = 'Distance vs. time in bike trips')
```


## Retain only data from April to November

In the cities of Bergen^[https://bergenbysykkel.no/en/faq] and Oslo^[https://oslobysykkel.no/en/how-it-works], bikes are available all year round.
In Trondheim, the bikes are available from April to the start of December^[https://trondheimbysykkel.no/en/faq]. 
We therefore focused our analysis on the months between April and November.

```{r filter_dates, results=FALSE}
# Create a new column with the month of each trip start date
data <- data %>%
  mutate(start_month = month(started_at))

# Count the number of rows in the original dataframe
before_date_filter <- nrow(data)

# Filter the trips for the months between April and November
data_filtered <- data %>%
  filter(start_month >= 4 & start_month <= 11)

# Count the number of rows in the filtered dataframe
after_date_filter <- nrow(data_filtered)

# Calculate the number of removed trips
row_removed_month <- before_date_filter - after_date_filter

data <- data_filtered
```
We removed `r row_removed_month` trips from our dataset.

We also added a column containing the day of the week as well as a column to indicate if the day belongs to the week-end. Official holidays are ignored for this analysis.

```{r weekday}
# Add a column for the day of the week
data$weekday <- weekdays(as.Date(data$started_at))
# Add a column
data$weekend <- ifelse(weekdays(as.Date(data$started_at)) %in% c("Saturday", "Sunday"), "Weekend", "Weekday")
```

## Number of stations

```{r num_stations, results=FALSE}
# Count the number of stations in each city
num_stations_per_city <- data %>%
  group_by(city) %>%
  summarize(num_stations = n_distinct(start_station_id)) %>%
  pull(num_stations)

# Store the number of stations per city in a separate variable
num_stations_bergen <- num_stations_per_city[1]
num_stations_oslo <- num_stations_per_city[2]
num_stations_trondheim <- num_stations_per_city[3]
```

The number of stations for each city is summarized in the following table:

City  | Number of stations
:-|:-
Bergen | `r num_stations_bergen`
Oslo | `r num_stations_oslo`
Trondheim | `r num_stations_trondheim`

```{r top_chunk, message=FALSE}
# Group the data by city and station ID, and calculate the number of trips
trips_by_station <- data %>%
  group_by(city, start_station_id) %>%
  summarize(num_trips = n())

# Sort the data by city and num_trips, and keep only the top n stations for each city
top_no <- 20

top_stations <- trips_by_station %>%
  arrange(city, desc(num_trips)) %>%
  group_by(city) %>%
  top_n(top_no)

# Filter the original dataset to keep only trips from the top stations
data_top_stations <- data %>%
  semi_join(top_stations, by = c("city", "start_station_id"))

before_top <- nrow(data)
after_top <- nrow(data_top_stations)
removed_top <- before_top - after_top

data <- data_top_stations
```

To narrow our analysis furthermore, we only considered the data coming from the top `r top_no` stations in each city.

We removed `r removed_top` trips from our dataset, which then contained `r after_top` trips.


## Time of rental

We reformated the columns containing the start and end time of the bike rentals.

```{r time_col}
data <- data %>% mutate(start_time = as.ITime(started_at)) %>%
  mutate(end_time = as.ITime(ended_at))
```

## Missing weather data

Given the large number of observations still available, we decided to not take into account trips with missing temperature values.

```{r missing_temp, results=FALSE}
data_filtered <- data %>% filter(!is.na(temp))

# Calculate the number of removed trips
removed_temp <- nrow(data) - nrow(data_filtered)

data <- data_filtered
```
We removed `r removed_temp` trips with missing temperature data.

After verification on the [meteostat.net website] (https://meteostat.net/en/), it seems that missing precipitations data corresponds to a value of 0.
We modified our dataset accordingly.

```{r precipitation}
data$prcp <- ifelse(is.na(data$prcp), 0, data$prcp)
```

# Data exploration

```{r trips_per_city_histo}
data %>% 
  ggplot(mapping = aes(x = city, fill = city)) + 
    geom_bar() +
  geom_text(aes(label = after_stat(count)), stat = 'count', nudge_y = 0.5,
    colour = 'black',size = 3) +
    labs(title = "Number of Trips per City")
```

```{r city_percent}
# Calculate the percentage of observations for each city
city_percentages <- data %>%
  group_by(city) %>%
  summarize(percentage = n() / nrow(data) * 100)

# Store the percentage for each city in separate variables
bergen_percentage <- city_percentages$percentage[city_percentages$city == "Bergen"]
trondheim_percentage <- city_percentages$percentage[city_percentages$city == "Trondheim"]
oslo_percentage <- city_percentages$percentage[city_percentages$city == "Oslo"]
```

The city with most of the trips in our dataset was Oslo (`r round(oslo_percentage,0)` % of the observations), followed by Bergen (`r round(bergen_percentage,0)` %). The data of the city of Trondheim represented `r round(trondheim_percentage,0)` % of our dataset.

```{r trips_per_month, message=FALSE}
data %>% 
  group_by(city, year_month = paste(format(started_at, "%Y-%m"), "-01", sep="")) %>% 
  summarise(n = n()) %>% 
  ggplot(mapping = aes(y = n, x = as.Date(year_month), group = city, colour = city)) + 
    geom_line() +
    geom_point() +
    labs(title = "Sum of Trips per City and Month", y = "Number of Trips", x = "")

# Extract the month from the "started_at" column
data$month <- month(data$started_at)

# Count the number of observations for each month
month_counts <- table(data$month)

# Find the month with the highest number of observations
max_month <- names(month_counts)[which.max(month_counts)]
```

The month with the most trips in our dataset was August.

## Time of the trips

```{r time_trips_plot, message=FALSE}
# Create a new variable for the hour of the day
data$Hour <- hour(data$started_at)

# Group the data by city and hour, and calculate the count of observations
hourly_counts <- data %>%
  group_by(city, Hour) %>%
  summarize(count = n())

# Create the plot
ggplot(hourly_counts, aes(x = Hour, y = count, color = city)) +
  geom_line() +
  geom_point() +
  labs(x = "Hour of the Day", y = "Number of Trips")

# Group the data by hour, and calculate the count of observations
hourly_counts2 <- data %>%
  group_by(Hour) %>%
  summarize(count = n())

# Find the three hours with the maximum count
top_hours <- hourly_counts2 %>%
  slice_max(order_by = count, n = 3) %>%
  pull(Hour)

# Store the three hours in separate variables
hour1 <- top_hours[1]
hour2 <- top_hours[2]
hour3 <- top_hours[3]
```

There are no missing values concerning the start or the end time of the trips.  
Our dataset contains bike trips throughout the day for almost every city. 
We could identify 2 peak times of usage: in the morning at `r hour2`am and in the afternoon between `r hour1`pm and `r hour3`pm.

## Number of trips per price

```{r price}
ggplot(data, aes(x = price, fill = city)) +
  geom_bar() +
  labs(x = "Price (NOK)", y = "Number of Trips", title = "Number of Trips by Price") +
  scale_fill_discrete(name = "City") +
  theme_minimal()

free_trips <- round(sum(data$duration <= 3600) / nrow(data) * 100,0)
```
Most of the trips last under 60 min (`r free_trips`%) and are free (included in the basis subscription).

## Weather

Evolution of the temperature and precipitations in Bergen, Oslo and Trondheim between April 2022 and November 2022.

### Temperature

```{r daily_temp, message=FALSE}
# Calculate the mean temperature per city and day
daily_temp <- data %>%
  mutate(started_day = as.Date(started_at)) %>%
  group_by(city, started_day) %>%
  summarize(mean_temp = mean(temp))

# Plot temperature by city
ggplot(daily_temp, aes(x = started_day, y = mean_temp, color = city)) +
  geom_line() +
  labs(x = "Date", y = "Mean Temperature", title = "Mean Temperature by City") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r data_temp}
# Find the row with the maximum mean temperature
max_temp_row <- daily_temp[daily_temp$mean_temp == max(daily_temp$mean_temp), ]

# Extract the city and started_at information from the row
city_max_temp <- max_temp_row$city
day_max_temp <- max_temp_row$started_day
max_temp <- round(max_temp_row$mean_temp,1)

mean_temp_Bergen <- round(mean(daily_temp$mean_temp[daily_temp$city == "Bergen"]),1)
mean_temp_Oslo <- round(mean(daily_temp$mean_temp[daily_temp$city == "Oslo"]),1)
mean_temp_Trondheim <- round(mean(daily_temp$mean_temp[daily_temp$city == "Trondheim"]),1)
```

The maximum temperature was reached in `r city_max_temp` with `r max_temp` on `r day_max_temp`.
The mean temperature is for each city is displayed in the following graph as a vertical line.

```{r temperature_trips}
ggplot(data, aes(x = temp, fill = city)) +
  geom_histogram(binwidth = 1, color = "black", position = "identity") +
  labs(x = "Temperature (°C)", y = "Number of Observations", title = "Number of Observations per Temperature") +
  geom_vline(aes(xintercept = mean_temp_Bergen), linetype = "dashed", color = "red", size = 1) +
  geom_vline(aes(xintercept = mean_temp_Oslo), linetype = "dashed", color = "green", size = 1) +
  geom_vline(aes(xintercept = mean_temp_Trondheim), linetype = "dashed", color = "blue", size = 1)
```
Most of the bike trips occured when the temperature was above 10°C.

### Precipitations

```{r precipitations, message=FALSE}
# Calculate the mean precipitation per hour and city
hourly_prcp <- data %>%
  group_by(city, Hour = format(started_at, "%H"), date = as.Date(started_at)) %>%
  summarize(mean_prcp = mean(prcp))

# Calculate the sum precipitation per day and city
daily_prcp <- hourly_prcp %>%
  group_by(city, date) %>%
  summarize(sum_prcp = sum(mean_prcp))

# Plot precipitation by city
ggplot(daily_prcp, aes(x = date, y = sum_prcp, color = city)) +
  geom_line() +
  labs(x = "Date", y = "Total Precipitation", title = "Total Precipitation per Day by City") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r precipitations_trips, message=FALSE, eval=FALSE}
 
ggplot(data, aes(x = prcp, fill = city)) +
  geom_histogram(binwidth = 1, color = "black", position = "identity") +
  labs(x = "Precipitations (mm)", y = "Number of Observations", title = "Number of Observations per Amount of precipitations")
```


# Linear Model: trip duration (Elise)

We first used a linear model using weather data (temperature and precipitation) as well as the day (weekday or weekend) to predict the duration of the trips.

```{r model_lm}
# Fit a linear regression model with duration as the response variable and weekend, distance, temperature and precipitations as predictor variables
model_lm <- lm(duration ~ h_distance + temp + prcp + poly(temp, degree=2) + poly(prcp, degree=2) + weekend, data = data)

# Summarize the results of the model
summary(model_lm)
```

All chosen predictor variables have a significant impact on the duration of the trips.
However, this model does not fit our data very well. The R^2 is of `r round(summary(model_lm)$r.squared,2)`.

```{r model_lm_predictions}
# Create a data frame with the observed and predicted values
pred_df <- data.frame(duration = data$duration,
                       predicted = predict(model_lm))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_df, aes(x = predicted, y = duration)) +
   geom_point() +
   geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
   labs(x = "Predicted Duration", y = "Observed Duration")
```

# Generalised Additive Model 

Another approach to predict the duration of the trips was to use a Generalise Additive Model (GAM), to study weather the  weather conditions affect the duration of trips.


```{r gam_dataset, message=FALSE}

#retrieve only data about the duration and weather parameters 
weather <- data %>% select(c(city, duration, temp, dwpt, prcp, snow, wpgt))
colnames(weather) <- c("city","Duration", "Temperature","Dew_Point", "Precipitation", "Snow_Depth", "Wind_Peak_Gust")

#remove NA values and chose relevant weather parameters
weather<- na.omit(weather[, c("city","Duration","Temperature", "Dew_Point", "Precipitation")])

head(weather)

#display relationship of duration with the weather variables
template.graph.weather <- ggplot(data = weather,
                                 mapping = aes(y = log(Duration))) +
  geom_point() +
  geom_smooth()

template.graph.weather + aes(x = Temperature)
template.graph.weather + aes(x = Dew_Point)
template.graph.weather + aes(x = Precipitation)
```
The distribution of temperature and dew point is evenly scattered through the graph. Whereas, the precipitation values are mainly available on the left side of the graph area. 
The graph of each variable seem to have a linear relationship that remains flat throughout the graph, therefore, an effect is difficult to spot here. 

```{r gam}
gam <- gam(log(Duration) ~ s(Temperature) + s(Dew_Point) + s(Precipitation), data = weather)

summary(gam)

plot(gam, residuals = TRUE, pages = 1, shade = TRUE)
```

The GAM model states that temperature, dew_point and precipitation have a significant effect on the duration of a bike trip. The smooth term temperature has the largest edf value `r round(summary(gam)$s.table["s(Temperature)", "edf"], 3)` and precipitation the lowest with a value of `r round(summary(gam)$s.table["s(Precipitation)", "edf"], 3)`. The intercept is existing. 

# Generalised Additive Model (Emilia) with same variables as linear model

Another approach to predict the duration of the trips was to use a Generalise Additive Model (GAM), to study how the  weather conditions affect the duration of trips.

```{r gam_dataset2, message=FALSE}

#retrieve only data about the duration and weather parameters 
weather <- data %>% select(c(duration, temp, prcp, h_distance, weekend))
colnames(weather) <- c("Duration", "Temperature", "Precipitation", "Distance", "Weekend")

#remove NA values and chose relevant weather parameters
weather<- na.omit(weather[, c("Duration", "Temperature", "Precipitation", "Distance", "Weekend")])

#display relationship of duration with the weather variables
template.graph.weather <- ggplot(data = weather,
                                 mapping = aes(y = log(Duration))) +
  geom_point() +
  geom_smooth()

template.graph.weather + aes(x = Temperature)
template.graph.weather + aes(x = Precipitation)
template.graph.weather + aes(x = Distance)
template.graph.weather + aes(x = Weekend)
```

```{r gam2}
gam <- gam(log(Duration) ~ s(Temperature) + s(Precipitation) + s(Distance) + Weekend,
              data = weather)
summary(gam)

plot(gam, residuals = TRUE, pages = 1, shade = TRUE)
```

The GAM model states that the temperature, the precipitations, the distance and the distinction weekday/weekend have a significant effect on the duration of a bike trip. We made the same observations with the linear model used above. However, the GAM performs better:the R^2 is of `r round(summary(gam)$r.squared,2)`.

# Generalised Linear Model with family set to Poisson (Pascal)

We wanted to predict the number of available bikes needed at a given station depending on the temperature. We used a Generalised Linear Model with the family Poisson to achieve this.  

Some further data preparation was needed. We calculated the number of outgoing trips at each station and kept the 6 stations with the most trips.
This model also takes into account the summer vacations between June 18th and August 21th.

```{r data_GLM_Poisson}
# Group the data by city and station ID, and calculate the number of trips
trips_by_station <- data_raw %>%
  group_by(city, start_station_id) %>%
  summarise(num_trips = n(), 
            city = last(city)) %>% 
  arrange(desc(num_trips)) 

# show stations with most trips
kable(head(trips_by_station, 6))
```

```{r outgoing_trips, message=FALSE}
# limit to top 6 stations
top_station_ids <- trips_by_station$start_station_id[1:6] 
summer_vacation_int <- interval(ymd("2022-06-18"), ymd("2022-08-21"))

trips_per_station <- data %>% 
  filter(start_station_id %in% top_station_ids) %>% 
  mutate(started_trip_date = as.Date(started_at)) %>% 
  group_by(start_station_id, started_trip_date) %>% 
  summarise(
      trips = n(),
      avg_temp = mean(temp, na.rm = T), # beware: this is not average temperature of the day, but avg temperature of trips
      max_temp = max(temp, na.rm = T),
      min_temp = min(temp, na.rm = T),
      avg_prcp = mean(prcp, na.rm = T)
    ) %>% 
  mutate(
      month = month(started_trip_date),
      weekday = lubridate::wday(started_trip_date, week_start = 1, label = T),
      weekend = lubridate::wday(started_trip_date, week_start = 1) >= 5,
      summer_vacation = started_trip_date %within% summer_vacation_int
    )
```

We plotted the correlation between the average daily temperature at the start of the trips at each station and the number of trips.

```{r glm_plot, message=FALSE}
trips_per_station %>%
  ggplot(mapping = aes(y = trips, x = avg_temp)) + 
  facet_wrap(~start_station_id) + 
  geom_point() +
  stat_smooth(method = "glm", method.args = list(family = 'poisson'), colour = '#F8766D') +
  labs(title = "Number of Trips vs. Average Temperature", 
       subtitle = 'At 6 most frequented stations',
       y = "Number of trips", 
       x = "Avg. temperature C°")
```


```{r}
trips_per_temp_glm <- glm(trips ~ avg_temp + factor(start_station_id)-1,
                     family = "poisson",
                     data = trips_per_station)
summary(trips_per_temp_glm)
```

```{r}
glm_temp_coef <- exp(coef(trips_per_temp_glm)['avg_temp'])

sprintf('For every increase in one temperature we get a %.2f times increase in trips a day.', glm_temp_coef)

```

__Interpretaiton of the GLM model output?? How well does it perform???__
For every increase of 1°C, we get a `r glm_temp_coef`% times increase in the number of trips a day.

<<<<<<< HEAD

# Generalised Linear Model with family set to Binomial 
=======
<<<<<<< HEAD
# Generalised Linear Model with family set to Binomial (Emilia)

=======

# Generalised Linear Model with family set to Binomial __OPEN__
>>>>>>> aca37a06d8b27b2756dece02e81690dd7f390060
>>>>>>> 2a770753e9bd8fcc98117e9018fd5f2c00fec4e0

```{r round_trip}
#new BOOLEAN variable: round_trip
data <- data %>% 
  mutate(round_trip = start_station_id == end_station_id)

# Count the proportions of observations with round_trip as TRUE
percent_round_trip <- round((sum(data$round_trip == TRUE) / nrow(data)) * 100,0)
```

We introduced a new variable in our dataset to analyze round trips: trips in which the user take and return a bike at the same station. Round trips represent `r percent_round_trip`% of our dataset. 

```{r round_plot}
data %>% 
  ggplot(mapping = aes(x = factor(start_station_id), fill = round_trip)) +
    geom_bar()
```

We use a generalized linear model with family set to binomial to analyze how the different cities, the distinction between weekday or weekend and the temperature influence the likellihood of a trip to be a roundtrip.
```{r binomial_round}
glm_binomial <- glm(round_trip ~ city - 1 + weekend + temp,
                     family = "binomial",
                     data = data)

summary(glm_binomial)
```

__INTERPRETATION__


```{r}
#extraction of the coefficient "weekend" for interpretation of the model

coef(glm_binomial)[c("weekendWeekend", "temp")]

exp(coef(glm_binomial)["weekendWeekend"])
```
All variables present to have significant coefficients, consequently, they all  highly impact the probability of a trip being a round trip. However, if we interpret individual coefficients such as the weekend/weekday, we can determine that the odds of a round trip on a weekend is 1.94 higher than on weekday.  

```{r}
#extraction of the coefficient temperature

exp.coef.temp <- exp(coef(glm_binomial)["temp"])
print(exp.coef.temp, digits = 3)


exp.coef.temp.10 <- exp(coef(glm_binomial)["temp"] * 10)
print(exp.coef.temp.10, digits = 3)
```

When increasing the temperature by 1°C, the odds of a round trip increase by 4%. 
To check the effect of the change of temperature, we wanted to test what would happen if we increase the temperature by 10°C. As we can see, the odds of a round trip increase to 55%.

# Neural Network (Pascal)

Data preparation for the neural net

```{r}
# remove obvious round trips
data_trips <- data %>% 
  filter(start_station_id != end_station_id) 

# select the predictors
predictors <- c("h_distance", "temp", "prcp")

# selec columns, drop NA and take random subsample of 100k trips
data_trips <- data_trips %>% 
  select(c("duration", predictors)) %>% 
  drop_na() %>% 
  sample_n(10000) 

# scale
max <- apply(data_trips, 2, max)
min <- apply(data_trips, 2, min)
data_trips_scaled <- as.data.frame(scale(data_trips, center = min, scale = max - min))

# split train/test
trainIndex <- createDataPartition(
  y = data_trips_scaled$duration, 
  p = .8,
  list = FALSE,
)
train_trips <- data_trips_scaled[trainIndex,]
test_trips <- data_trips_scaled[-trainIndex,]
```

Train baseline model, hyperparams more or less random

```{r}
# train the neural network
trips_net <- neuralnet(duration ~ h_distance, data = train_trips, linear.output = TRUE, hidden = c(4,3))

# predict results on test-set
prediction <- compute(trips_net, test_trips)

plot(trips_net)
```

see predictions of baseline model

```{r}
# calculate RMSE
rmse <- sqrt(mean((test_trips$duration - prediction$net.result)^2))

# plot prediction vs real
plot(test_trips$duration, prediction$net.result, col='blue', pch=16, ylab = "predicted duration", xlab = "real duration", main = 'Baseline Model (haversine distance)')

# plot perfect prediction
abline(0,1)

# add RMSE to plot
mtext(paste("RMSE: ", round(rmse, digits = 3)), side = 3)
```

Add temp and precip as predictors to model

```{r}
# train
trips_net_2 <- neuralnet(duration ~ h_distance + temp + prcp, data = train_trips, linear.output = TRUE, hidden = c(4,3))

# make predictions
prediction_2 <- compute(trips_net_2, test_trips)

# plot second model
plot(trips_net_2)
```

see predictions of second model

```{r}
# calculate RMSE
rmse_2 <- sqrt(mean((test_trips$duration - prediction_2$net.result)^2))

<<<<<<< HEAD
plot(gam, residuals = TRUE, pages = 1, shade = TRUE)
=======
# plot prediction vs real
plot(test_trips$duration, prediction_2$net.result, col='blue', pch=16, ylab = "predicted duration", xlab = "real duration", main = 'Model 2 (h-distance, temp, precip)')

# plot perfect prediction
abline(0,1)

# add RMSE to plot
mtext(paste("RMSE: ", round(rmse_2, digits = 3)), side = 3)
>>>>>>> c88c0d86b73b77ae880ceb944c32123cd8238302
```

does not make any difference. Check with hyperparameter-tuning (caret, see ANN_Lab3_Cereals) to get best paramaters

maybe rescale results got get better idea of values

# Support Vector Machine

We tested if a Support Vector Machine could help predict the city based on the latitude and longitude of the stations.
This could for example be useful to adopt a nation-wide bike-sharing system without relying on station ID to identify the city of the station.

```{r cities_dataset}
# prepare data
cities_start <- data %>% select(c(start_station_latitude, start_station_longitude, city))
colnames(cities_start) <- c("latitude", "longitude","city")

cities_end <- data %>% select(c(end_station_latitude, end_station_longitude, city))
colnames(cities_end) <- c("latitude", "longitude","city")

cities <- rbind(cities_start, cities_end)
rm(cities_start, cities_end)

cities$city <- as.factor(cities$city)

print(colSums(is.na(cities))) # no missing values!

set.seed(10)
cities_subset <- rbind(
     cities[city == 'Trondheim'][sample(nrow(cities[city == 'Trondheim']), 33300), ],
  cities[city == 'Oslo'][sample(nrow(cities[city == 'Oslo']), 33300), ],
  cities[city == 'Bergen'][sample(nrow(cities[city == 'Bergen']), 33300), ]
)
cities_subset %>% 
  group_by(city) %>% 
  summarise('number' = n())
```

```{r svm_plot}
cities_subset %>% 
  ggplot(mapping = aes(x = longitude, y = latitude, color = city)) +
    geom_point() +
    labs(title = "Start/End points per city")
```


```{r cities_dataset_split}
# form training and testing datasets
set.seed(10)
indices <- createDataPartition(cities_subset$city, p=0.7, list=F)

train_cities <- cities_subset %>% slice(indices)
test_cities <-  cities_subset %>% slice(-indices)
test_cities_truth <- cities_subset %>% slice(-indices) %>% pull(city)
```

```{r SVM_city_linear}
cities_svm <- svm(city ~. , train_cities, kernel = "linear", scale = TRUE, cost = 10)

#make predictions
test_pred <- predict(cities_svm, test_cities)
table(test_pred)
```

```{r}
plot(cities_svm, train_cities, latitude ~ longitude)
```

```{r}

#evaluate the results
conf_matrix <- confusionMatrix(test_pred, test_cities_truth)
conf_matrix
```

No data points was misclassified. We built a SVM with a prediction accuracy of 100%.

# Solve an optimisation problem __OPEN__

# Conclusion
Our analysis focused on data obtained from the cities of Bergen, Oslo, and Trondheim in Norway. The initial phase of the project involved data preparation to facilitate its utilization in the subsequent models we employed.

To begin with, we employed a linear model to examine the potential impact of variables such as the weekend, temperature, distance, and precipitation on the duration of bike rides. However, due to the model's inadequate fit, we decided to employ a GAM using the same variables. The results from the GAM model substantiated our initial suspicions derived from the linear model. As the R-squared value increased in the GAM model, the reliability of the results improved. We hope these findings can provide the bike rental company with a comprehensive understanding of the factors that influence their business, aiding them in their strategical decisions. Considering that revenue is generated only after 60 minutes of bike riding, identifying the factors that potentially affect profitability becomes highly relevant for the company.

Given the inherent association between weather conditions and bike riding, we conducted a further investigation in this regard. Consequently, we employed a GAM model specifically targeting the most promising weather variables. Once again, the results affirmed that these factors indeed affect the duration of bike rides. Such insights can assist the company in determining the types of bikes required for shorter or longer trips based on wearther conditions forecast. 

Furthermore, it was imperative for us to provide the company with information enabling them to ascertain the demand for bikes. For this purpose, we employed a GLM with family set to Poisson. The results indicated a positive correlation between temperature and bike demand, implying that higher temperatures lead to higher usage of bikes. This information proves useful in avoiding shortages and accurately predicting the required number of parking slots at bike stations, thereby potentially maximizing profit and reducing costs.

Moreover, we sought to determine whether factors such as weekends or temperature influenced whether bike users would return to the same station from which they initially rented the bike. The results indicated that these factors do indeed impact return behavior. Once again, this information can serve as an additional metric for estimating demand on specific weekdays.

#Neural network and SVM insights


Overall, our analysis provides valuable insights to the bike rental company, shedding light on the factors affecting ride duration, bike demand, and return behavior. Armed with this knowledge, the company can make informed decisions to optimize operations, enhance customer satisfaction, and maximize profitability.
