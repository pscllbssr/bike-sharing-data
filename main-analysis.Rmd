---
title: "Analysis of bike sharing data"
author: "Elise Gourri, Emilia Marlene Ribeiro Peixoto, Pascal Albisser"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: TRUE
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

__the person who corrected my bootcamp project really loved the fact that we described what we were loading the libraries for. Do you agree to add a little structure and make some comments in it as well, as I did in the r libraries chunk?__


```{r libraries, message = FALSE, echo = FALSE}

# data manipulation and visualization
library(data.table) # aggregation of large data
install.packages("tidyverse")
library(tidyverse)
library(lubridate)
library(kableExtra) # to render tables in html
install.packages("gridExtra")
library(gridExtra)
install.packages("ggplot2")
library(ggplot2)
library(dplyr)

# machine learning libraries
install.packages("e1071")
library(e1071)
install.packages("caret")
library(caret) 
install.packages("kernlab")
library(kernlab)
library(mgcv)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = function() {
  if (knitr::is_latex_output())
    "latex" else "pipe"
})
```

# Dataset

__Map of Norway with the bike sharing logos indicating each city.__

Our Dataset, pls run `scripts/download-data.R` first.

__Data from when to when?__
__Data sources__


__Main variables:__ __TO BE COMPLETED__  
- Time of start and end of the trip, as well as its duration (in seconds)
- Start and end stations
- City
- Temperature
- Wind
- Amount of precipitation
- Presence or absence of snow


```{r head_data}
data <- fread("data/trips_s.csv")
kable(head(data))
```

__show this graphes in data exploration?__

```{r trips_per_city}
data %>% 
  ggplot(mapping = aes(x = city, fill = city)) + 
    geom_bar() +
  geom_text(aes(label = after_stat(count)), stat = 'count', nudge_y = 0.5,
    colour = 'black',size = 3) +
    labs(title = "Number of trips per city")
```


```{r trips_per_month}
data %>% 
  group_by(city, year_month = paste(format(started_at, "%Y-%m"), "-01", sep="")) %>% 
  summarise(n = n()) %>% 
  ggplot(mapping = aes(y = n, x = as.Date(year_month), group = city, colour = city)) + 
    geom_line() +
    geom_point() +
    labs(title = "Sum of trips per city and month", y = "number of trips", x = "")
```

## Data preparation

We need to modify the dataset to be able to perform our analysis.

### Duration of trips -TO RUN-

```{r duration_nb, results=FALSE}
# count the number of trips that lasted 2 minutes or less and starting and ending at the same station
short_trips <- nrow(subset(data, duration <= 120 & start_station_id == end_station_id))
short_trips
# count the number of trips that lasted 24 hour and more
long_trips <- nrow(subset(data, duration >= 86400))
long_trips

# remove the data concerning the "short" and long "trips"
data <- data[!(data$duration <= 120 & data$start_station_id == data$end_station_id) & !(data$duration >= 86400), ]

max(data$duration)
```

The duration of each trips is indicated in seconds.   

We removed `r short_trips` trips which lasted 2 min or less with the same start and end station. They probably correspond to users picking up a bike and immediately returning it. We also focused on trips lasting less than 24 hours 
and therefore removed `r long_trips` trips from our dataset.

__box plot is not the best... we need to do it better__
```{r duration_plot, eval=FALSE}
data_mod %>% 
  ggplot(mapping = aes(x = city, y = duration)) + 
    geom_boxplot() +
    labs(title = "Distribution of the trips duration")
```

### Retain only April to November -TO RUN-

In Trondheim, the bikes are available as long as there is no ice on the ground, 
from April to the start of December. ^[https://trondheimbysykkel.no/en/faq]  
We therefore focus our analysis on the months between April and November.

```{r filter_dates, results=FALSE}
# Create a new column with the month of each trip start date
data <- data %>%
  mutate(start_month = month(started_at))

# Count the number of rows in the original dataframe
before_date_filter <- nrow(data)
before_date_filter

# Filter the trips for the months between April and November
data_filtered <- data %>%
  filter(start_month >= 4 & start_month <= 11)

# Count the number of rows in the filtered dataframe
after_date_filter <- nrow(data_filtered)
after_date_filter

# Calculate the number of removed trips
row_removed <- before_date_filter - after_date_filter
row_removed
```

We removed `r row_removed` trips from our dataset.

We also added a column containing the day of the week as well as a column to indicate if the day belongs to the week-end. Official holydays are ignored for this analysis.

```{r weekday}
# Add a column for the day of the week
data$weekday <- weekdays(as.Date(data$started_at))
# Add a column 
data$weekend <- ifelse(weekdays(as.Date(data$started_at)) %in% c("Saturday", "Sunday"), "Weekend", "Weekday")
```

### Number of stations -TO RUN-

```{r num_stations, results=FALSE}
# Count the number of stations in each city
num_stations_per_city <- data %>%
  group_by(city) %>%
  summarize(num_stations = n_distinct(start_station_id)) %>%
  pull(num_stations)

# Store the number of stations per city in a separate variable
num_stations_bergen <- num_stations_per_city[1]
num_stations_oslo <- num_stations_per_city[2]
num_stations_trondheim <- num_stations_per_city[3]
```
The number of stations for each city is summarized in the following table:

City  | Number of stations
-|-
Bergen | `r num_stations_bergen`
Oslo | `r num_stations_oslo`
Trondheim | `r num_stations_trondheim`

To narrow our analysis furthermore, we will only consider the data coming from the top `r top` stations in each city.

```{r top, results = FALSE}
# Group the data by city and station ID, and calculate the number of trips
trips_by_station <- data %>%
  group_by(city, start_station_id) %>%
  summarize(num_trips = n()) 

# Sort the data by city and num_trips, and keep only the top n stations for each city
top = 20

top_stations <- trips_by_station %>%
  arrange(city, desc(num_trips)) %>%
  group_by(city) %>%
  top_n(top)

# Filter the original dataset to keep only trips from the top stations
data_top_stations <- data %>%
  semi_join(top_stations, by = c("city", "start_station_id"))

before_top20 <- nrow(data)
after_top20 <- nrow(data_top_stations)
removed_top20 <- before_top20 - after_top20

data <- data_top_stations
```
We removed `r removed_top20` trips from our dataset, which then contained `r after_top20` trips.

### Time of rental

```{r time_col, eval=false}
data_mod <- data %>% mutate(start = as.ITime(started_at)) %>%
  mutate(end = as.ITime(ended_at))
head(data_mod)
```

### Missing weather data -TO RUN-

Given the large number of observations still available, we decided to not take into account trips with missing temperature values.

```{r missing_temp, results=FALSE}

data_filtered <- data %>% filter(!is.na(temp))

# Calculate the number of removed trips
removed_temp <- nrow(data) - nrow(data_filtered)
removed_temp

data <- data_filtered
```

We removed `r removed_temp` trips with missing temperature data.

After verification on the metesostat.net website, it seems that missing precipitations data corresponds to a value of 0.
We modified our dataset accordingly.

```{r precipitation}
data$prcp <- ifelse(is.na(data$prcp), 0, data$prcp)
```


## Time of the trips

There are no missing values concerning the start or the end time of the trips.

Some background information: __may or may not be included in the report at the end__
In Bergen, bikes are available for rental throughout the year between 5am and midnight. ^[https://bergenbysykkel.no/en/faq]
The bikes in the city of Oslo are available anytime of the year between 5am and 1am. ^[https://oslobysykkel.no/en/faq]
In Trondheim, the bikes are available as long as there is no ice on the ground, 
from April to the start of December. ^[https://trondheimbysykkel.no/en/faq]

__I (Elise) want to check that this info fits our data.__

```{r time_trips, eval=FALSE}

# Bergen
# There should be no data between midnight and 5am.
bergen <- data_mod %>% filter(city == "Bergen")
bergen %>% ggplot(aes(x = start)) +
  geom_histogram() + 
  scale_x_time(breaks = "1 hour")

# Oslo
# There should be no data between 1am and 5am.
oslo <- data %>% filter(city == "Oslo")

# Trondheim
# There should be no data for the months January to March. and some data available during the night.
trondheim <- data %>% filter(city == "Trondheim")
```


# Weather (Elise)

__graphes of temperature in each city btw april and november and same with precipitations___

I started using our main dataset but because I want to summarize data per day. 
I don't know how to group the data because we have a different amount of data per hour 
with the bike rental data...
So for now I will try with the weather_data dataframe. We can ditch all this later if 
we don't use it.

## Precipitations

__Here, I am checking if the na in our weather data precipitations are also na on the metesostat.net website.I checked 2021-08-30 3h Oslo: on the website, it is everywhere 0. So we should be fine replacing the NA by 0 for precipitation. -> see above: NA are now zeros.__

```{r precip-na, eval=FALSE}
precip_na <- filter(weather_data,is.na(weather_data$prcp)) %>% arrange(desc(date))
head(precip_na,500)
```

```{r precipitations, message = FALSE}
# precip <- weather_data %>% filter(date > "2017-12-31") %>%
#   mutate(prcp = replace_na(prcp, 0)) %>%
#   group_by(city, year_month = as.Date(paste(format(date, "%Y-%m"), "-01", sep=""))) %>% 
#   summarise(precipitations = sum(prcp)) 
# 
# ggplot(data = precip, mapping = aes(y = precipitations, x = year_month, group = city, colour = city)) + 
#   geom_point() +
#   geom_line() +
#   labs(title = "Sum of precipitations per city and month", y = "precipitations in mm", x = "")
```


# Linear Model (Elise)

__I am doing everything step-by-step and super detailed for now, but will only keep one model at the end__

## Is the duration of the trip and impacted by the weather?

We start with a linear regression to explore the relationship between the duration of the trip
and the temperature and precipitation. 

how much of the variation in trip duration can be explained by changes in temperature and precipitation?

We first plot the duration versus temperature and precipitation with a linear regression line.

```{r simple_lm_plot, fig.show='hold', out.width="49%"}

# Create the first plot: temperature vs. duration
plot1 <- ggplot(data, aes(x = temp, y = duration)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Temperature", y = "Duration")

# Create the second plot: precipitation vs. duration
plot2 <- ggplot(data, aes(x = prcp, y = duration)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Precipitation", y = "Duration")

# Display the two plots side-by-side
gridExtra::grid.arrange(plot1, plot2, ncol = 2)
```

We combined these 2 predictors (temperature and precipitations) in one model.

```{r simple_lm}
# Fit a simple linear regression model with duration as the response variable and temperature and precipitation as predictor variables
simple_lm <- lm(duration ~ temp + prcp, data = data)

# Summarize the results of the model
summary(simple_lm)
```
The temperature seems to have a significant positive correlation with the duration of trips and rain seems to correlate with a reduced duration of trips. While the model is significant, it does not fit the dataset optimally: the R^2 is only of `r summary(simple_lm)$r.squared`.
<<<<<<< HEAD

When including the interaction between the temperature and precipitations, the model looks as follow:

```{r simple_lm_interaction}
# Fit a simple linear regression model with duration as the response variable and temperature and precipitation as predictor variables, including their interaction
simple_lm_interaction <- lm(duration ~ temp * prcp, data = data)

# Summarize the results of the model
summary(simple_lm_interaction)
```

The interaction between the temperature and precipitations shows no significance.

=======

When including the interaction between the temperature and precipitations, the model looks as follow:

```{r simple_lm_interaction}
# Fit a simple linear regression model with duration as the response variable and temperature and precipitation as predictor variables, including their interaction
simple_lm_interaction <- lm(duration ~ temp * prcp, data = data)

# Summarize the results of the model
summary(simple_lm_interaction)
```

The interaction between the temperature and precipitations shows no significance.

>>>>>>> 819775ec1ae10569ce7cb8396f83b4905934c1d4
```{r simple_lm_prediction}
# Create a data frame with the observed and predicted values
pred_df <- data.frame(duration = data$duration,
                      predicted = predict(simple_lm))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_df, aes(x = predicted, y = duration)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
# <<<<<<< HEAD
  labs(x = "Predicted duration", y = "Observed duration")
```

The relationship between temperature, precipitations and trip durations is not linear.
We can try non-linear relationships such as polynomials.

```{r quad_lm}
# Fit a quadratic model 
quad_lm <- lm(duration ~ temp + prcp + poly(temp, degree=2) + poly(prcp, degree=2), data = data)
# Print the model summary
summary(quad_lm)

# F-test between quadratic model and simple linear model
anova(simple_lm, quad_lm)
```

```{r  plot_quad_lm}
# Create a data frame with the observed and predicted values
pred_quad_df <- data.frame(duration = data$duration,
                      predicted = predict(quad_lm))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_quad_df, aes(x = predicted, y = duration)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
  labs(x = "Predicted duration", y = "Observed duration")
```

=======
  labs(x = "Predicted duration", y = "Observed duration")
```

The relationship between temperature, precipitations and trip durations is not linear.
We can try non-linear relationships such as polynomials.

```{r quad_lm}
# Fit a quadratic model 
quad_lm <- lm(duration ~ temp + prcp + poly(temp, degree=2) + poly(prcp, degree=2), data = data)
# Print the model summary
summary(quad_lm)

# F-test between quadratic model and simple linear model
anova(simple_lm, quad_lm)
```

```{r  plot_quad_lm}
# Create a data frame with the observed and predicted values
pred_quad_df <- data.frame(duration = data$duration,
                      predicted = predict(quad_lm))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_quad_df, aes(x = predicted, y = duration)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
  labs(x = "Predicted duration", y = "Observed duration")
```

>>>>>>> 819775ec1ae10569ce7cb8396f83b4905934c1d4
I just want to see what type of relationships are present and let r plot the smooth line without indicating a method.

```{r lm_plots, fig.show='hold', out.width="49%"}
# Create the first plot: temperature vs. duration
plot1 <- ggplot(data, aes(x = temp, y = duration/60)) +
  geom_point() +
  geom_smooth() +
  labs(x = "Temperature", y = "Duration") +
  coord_cartesian(ylim = c(0, 60))

# Create the second plot: precipitation vs. duration
plot2 <- ggplot(data, aes(x = prcp, y = duration/60)) +
  geom_point() +
  geom_smooth() +
  labs(x = "Precipitation", y = "Duration") +
  coord_cartesian(ylim = c(0, 60))

# Display the two plots side-by-side
gridExtra::grid.arrange(plot1, plot2, ncol = 2)
```

## Effect of the week-end on the trip duration

We want to evaluate if the trips during the week-end are longer than the trips during the week. We visualized this effect using a box-plot zooming on trips duration under 1 hour.

```{r duration_week-end_plots}
# Create a box plot of trip duration by weekday or week-end, zooming in the trip durations under 3600 seconds
ggplot(data, aes(x = weekend, y = duration/60)) +
  geom_boxplot() +
  labs(x= "", y = "Trip Duration (minutes)") +
  coord_cartesian(ylim = c(0, 60))
```
Let's integrate it in our linear model:
```{r quad_lm_weekend}
# Fit a quadratic model 
quad_lm_weekend <- lm(duration ~ temp + prcp + poly(temp, degree=2) + poly(prcp, degree=2) + weekend, data = data)
# Print the model summary
summary(quad_lm_weekend)

# F-test between quadratic model and simple linear model
anova(quad_lm, quad_lm_weekend)
```
The R^2 value is a bit better than before.

```{r  plot_quad_lm_weekend}
# Create a data frame with the observed and predicted values
pred_quad_weekend_df <- data.frame(duration = data$duration,
                      predicted = predict(quad_lm_weekend))

# Create a scatter plot of observed vs. predicted values
ggplot(pred_quad_weekend_df, aes(x = predicted, y = duration/60)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "blue") +
  labs(x = "Predicted duration", y = "Observed duration")
```


# Generalised Linear Model with family set to Poisson

predict how many bikes we need to have available at given station, depending on weather?, ...?

trips is count data, so we need poisson model.

see which stations have the most trips:

```{r maxtrips_station}
data %>% 
  group_by(start_station_id) %>% 
  summarise(
    outgoing_trips=n(),
    city=last(city),
    lat=last(start_station_latitude),
    lon=last(start_station_longitude)) %>% 
  arrange(desc(outgoing_trips)) %>% 
  head(10)
```

Seems to be station in the city center, close to university: <http://www.openstreetmap.org/?mlat=60.37969&mlon=5.351994&zoom=12>

prepare data

```{r station220_data, message = False}
# get data for station 220
station_220_data <- data %>% 
  filter(start_station_id == 220)

# get outgoing trips per day and possible predictors
station_220_data <- station_220_data %>% 
  mutate(started_trip_date = as.Date(started_at)) %>% 
  group_by(started_trip_date) %>% 
  summarise(
    trips = n(),
    avg_temp = mean(temp, na.rm = T),
    max_temp = max(temp, na.rm = T),
    min_temp = min(temp, na.rm = T),
  ) %>% 
  mutate(weekday = wday(started_trip_date, week_start=1)) 

# Removing any missing values:
station_220_data <- na.omit(station_220_data)
```
see correlation with temperatures

```{r station220_avgtemp}
station_220_data %>% 
  ggplot(mapping = aes(y = trips, x = avg_temp)) + 
      geom_point() +
      labs(title = "Number of trips vs. average temperature", 
           subtitle = "station 220, Bergen",
           y = "number of trips", 
           x = "avg. temperature C°")

```

fit the model

```{r station220_avgtemp_poisson} 
trips_at_220_model <- glm(trips ~ avg_temp,
  family = "poisson",
  data = station_220_data)

summary(trips_at_220_model)
```

beware this is tons of samples, high power

but result is highly significant

```{r station220_avgtemp_poisson_graphe}
station_220_data %>% 
  ggplot(mapping = aes(y = trips, x = avg_temp)) + 
      geom_point() +
      labs(title = "Number of trips vs. average temperature", 
           subtitle = "station 220, Bergen",
           y = "number of trips", 
           x = "avg. temperature C°") +
      geom_abline(intercept = trips_at_220_model$coefficients[1], slope = trips_at_220_model$coefficients[2])
```
## Similar analysis with maximal temperature (Elise)

I thought using the maximal temperature would make a difference but it does not :).
__Would making this study on the top 10 stations or on a whole city make a difference?__

```{r station220_maxtemp}
station_220_data %>% 
  ggplot(mapping = aes(y = trips, x = max_temp)) + 
      geom_point() +
      labs(title = "Number of trips vs. average temperature", 
           subtitle = "station 220, Bergen",
           y = "number of trips", 
           x = "Max temperature C°")
```
```{r station220_maxtemp_poisson}
station_220_data <- na.omit(station_220_data)
trips_at_220_model_maxtemp <- glm(trips ~ max_temp,
  family = "poisson",
  data = station_220_data)

summary(trips_at_220_model_maxtemp)
```
```{r station220_maxtemp_poisson_graphe}
station_220_data %>% 
  ggplot(mapping = aes(y = trips, x = max_temp)) + 
      geom_point() +
      labs(title = "Number of trips vs. maximal temperature", 
           subtitle = "station 220, Bergen",
           y = "number of trips", 
           x = "max temperature C°") +
      geom_abline(intercept = trips_at_220_model_maxtemp$coefficients[1], slope = trips_at_220_model_maxtemp$coefficients[2])
```


# Generalised Linear Model with family set to Binomial

# Generalised Additive Model
Do weather conditions affect the duration of trips? 
 
```{r}
install.packages("dplyr")
library(dplyr)

#retrieve only data about the duration and weather parameters 
weather <- trip %>% select(c(city,duration, temp, dwpt, prcp, snow, wpgt))
colnames(weather) <- c("city","Duration", "Temperature","Dew_Point", "Precipitation", "Snow_Depth", "Wind_Peak_Gust")
head(weather)

#remove NA values and chose relevant weather parameters
weather_01<- na.omit(weather[, c("city","Duration","Temperature", "Dew_Point", "Precipitation")])
str(weather_01)
head(weather_01)

#display relationship of duration with the weather variables
template.graph.weather <- ggplot(data = weather_01,
                                 mapping = aes(y = log(Duration))) +
  geom_point() +
  geom_smooth()

template.graph.weather + aes(x = Temperature) 
template.graph.weather + aes(x = Dew_Point)
template.graph.weather + aes(x = Precipitation)



```

```{r}
gam_01 <- gam(log(Duration) ~ s(Temperature) + s(Dew_Point) + s(Precipitation),
              data = weather_01)
summary(gam_01)

plot(gam_01, residuals = TRUE, pages = 1, shade = TRUE)
```

The GAM model states that temperature and dew_point have a significant effect on the duration of a bike trip. Whereas, the precipitation doesn't seem to have significant impact. The temperature data is rather normal distributed, the dew_point is left-side skewed and precipitation is right-skewed. 

# Neural Network

# Support Vector Machine

We predict the city based on latitude and longitude.

```{r cities_dataset}
# prepare data
cities_start <- data %>% select(c(start_station_latitude, start_station_longitude, city))
colnames(cities_start) <- c("latitude", "longitude","city")

cities_end <- data %>% select(c(end_station_latitude, end_station_longitude, city))
colnames(cities_end) <- c("latitude", "longitude","city")

cities <- rbind(cities_start, cities_end)
rm(cities_start, cities_end)

cities$city <- as.factor(cities$city)

print(colSums(is.na(cities))) # no missing values!

set.seed(10)
cities_subset <- rbind(
  cities[city == 'Trondheim'][sample(nrow(cities[city == 'Trondheim']), 33300), ],
  cities[city == 'Oslo'][sample(nrow(cities[city == 'Oslo']), 33300), ],
  cities[city == 'Bergen'][sample(nrow(cities[city == 'Bergen']), 33300), ]
)
cities_subset %>% 
  group_by(city) %>% 
  summarise('number' = n())
  
```

```{r}
cities_subset %>% 
  ggplot(mapping = aes(x = longitude, y = latitude, color = city)) +
    geom_point() +
    labs(title = "Start/End points per city")
```


```{r cities_dataset_split}
# form training and testing datasets
set.seed(10)
indices <- createDataPartition(cities_subset$city, p=0.7, list=F)

train_cities <- cities_subset %>% slice(indices)
test_cities <-  cities_subset %>% slice(-indices)
test_cities_truth <- cities_subset %>% slice(-indices) %>% pull(city)
```

## Linear kernel

```{r SVM_city_linear}
cities_svm <- svm(city ~. , train_cities, kernel = "linear", scale = TRUE, cost = 10)

#make predictions
test_pred <- predict(cities_svm, test_cities)
table(test_pred)
```

```{r}
plot(cities_svm, train_cities, latitude ~ longitude)
```

```{r}
#evaluate the results
conf_matrix <- confusionMatrix(test_pred, test_cities_truth)
conf_matrix

```

Only 3 data points were misclassified. We built a SVM with a prediction accuracy of nearly 100%.
__cf. Iris lab: We should do some cross-validation to ensure that we are not simply lucky with the train/test split. But on the other end... it is good enough, no?__

## Radial kernel

```{r SVM_city_radial}
cities_svm_radial <- svm(city ~. , train_cities, kernel = "radial", scale = TRUE, cost = 100)

# plot classification
plot(cities_svm_radial, train_cities, latitude ~ longitude)
```

```{r}
#make predictions
test_pred2 <- predict(cities_svm_radial, test_cities)
table(test_pred2)
```

```{r}
#evaluate the results
conf_matrix2 <- confusionMatrix(test_pred2, test_cities_truth)
conf_matrix2
```
Same results with another type of kernel.

__We ignore the 3 wrong values (at least for now :)__ 

# Solve an optimisation problem
