---
title: "Analysis of bike sharing data"
author: "Elise Gourri, Emilia Marlene Ribeiro Peixoto, Pascal Albisser"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: TRUE
    code_folding: hide
---

__the person who corrected my bootcamp project really loved the fact that we described what we were loading the libraries for. Do you agree to add a little structure and make some comments in it as well, as I did in the r libraries chunk?__


```{r libraries, message = FALSE, echo = FALSE}

# data manipulation and visualization
library(data.table) # aggregation of large data
library(tidyverse)
library(kableExtra) # to render tables in html

# machine learning libraries
library(e1071)
library(caret) 
library(kernlab)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = function() {
  if (knitr::is_latex_output())
    "latex" else "pipe"
})
```

# Dataset

Our Dataset, pls run `scripts/download-data.R` first.

__Main variables:__ __TO BE COMPLETED__  
- Time of start and end of the trip, as well as its duration (in seconds)
- Start and end stations
- City
- Temperature
- Wind
- Amount of precipitation
- Presence or absence of snow

```{r head_data}
data <- fread("data/trips_s.csv")
kable(head(data))
```


```{r trips_per_city}
data %>% 
  ggplot(mapping = aes(x = city, fill = city)) + 
    geom_bar() +
  geom_text(aes(label = after_stat(count)), stat = 'count', nudge_y = 0.5,
    colour = 'black',size = 3) +
    labs(title = "Number of trips per city")
```


```{r trips_per_month, message = FALSE}
data %>% 
  group_by(city, year_month = paste(format(started_at, "%Y-%m"), "-01", sep="")) %>% 
  summarise(n = n()) %>% 
  ggplot(mapping = aes(y = n, x = as.Date(year_month), group = city, colour = city)) + 
    geom_line() +
    geom_point() +
    labs(title = "Sum of trips per city and month", y = "number of trips", x = "")
```

## Modification of the dataset

We need to modify the dataset to be able to perform our analysis.

### Time of rental

```{r time_col}
data_mod <- data %>% mutate(start = as.ITime(started_at)) %>%
  mutate(end = as.ITime(ended_at))
head(data_mod)
```


## Time of the trips

There are no missing values concerning the start or the end time of the trips.

Some background information: __may or may not be included in the report at the end__
In Bergen, bikes are available for rental throughout the year between 5am and midnight. ^[https://bergenbysykkel.no/en/faq]
The bikes in the city of Oslo are available anytime of the year between 5am and 1am. ^[https://oslobysykkel.no/en/faq]
In Trondheim, the bikes are available as long as there is no ice on the ground, 
from April to the start of December. ^[https://trondheimbysykkel.no/en/faq]

__I (Elise) want to check that this info fits our data.__

```{r time_trips}

# Bergen
# There should be no data between midnight and 5am.
bergen <- data_mod %>% filter(city == "Bergen")
bergen %>% ggplot(aes(x = start)) +
  geom_histogram() + 
  scale_x_time(breaks = "1 hour")

# Oslo
# There should be no data between 1am and 5am.
oslo <- data %>% filter(city == "Oslo")

# Trondheim
# There should be no data for the months January to March. and some data available during the night.
trondheim <- data %>% filter(city == "Trondheim")





```



## Duration of trips

```{r duration_nb, results = FALSE}
short_trips <- nrow(subset(data, duration <= 120 & start_station_id == end_station_id))
short_trips
long_trips <- nrow(subset(data, duration >= 86400))
long_trips

data_mod <- data_mod[!(data_mod$duration <= 120 & data_mod$start_station_id == data_mod$end_station_id) & !(data_mod$duration >= 86400), ]

max(data_mod$duration)
```

The duration of each trips is indicated in seconds.   
We removed `r short_trips` trips 
which lasted 2 min or less with the same start and end station. They probably correspond 
to users picking up a bike and immediately returning it. We also focused on trips lasting less than 24 hours 
and therefore removed `r long_trips` trips from our dataset.

__box plot is not the best... we need to do it better__
```{r duration_plot}
data_mod %>% 
  ggplot(mapping = aes(x = city, y = duration)) + 
    geom_boxplot() +
    labs(title = "Distribution of the trips duration")
```


# Weather (Elise)

I started using our main dataset but because I want to summarize data per day. 
I don't know how to group the data because we have a different amount of data per hour 
with the bike rental data...
So for now I will try with the weather_data dataframe. We can ditch all this later if 
we don't use it.

## Precipitations

Here, I am checking if the na in our weather data precipitations are also na on the metesostat.net website.
I checked 2021-08-30 3h Oslo: on the website, it is everywhere 0. So we should be fine replacing the NA by 0 for precipitation.

```{r precip-na}
# precip_na <- filter(weather_data,is.na(weather_data$prcp)) %>% arrange(desc(date))
# head(precip_na,500)
```

```{r precipitations, message = FALSE}
# precip <- weather_data %>% filter(date > "2017-12-31") %>%
#   mutate(prcp = replace_na(prcp, 0)) %>%
#   group_by(city, year_month = as.Date(paste(format(date, "%Y-%m"), "-01", sep=""))) %>% 
#   summarise(precipitations = sum(prcp)) 
# 
# ggplot(data = precip, mapping = aes(y = precipitations, x = year_month, group = city, colour = city)) + 
#   geom_point() +
#   geom_line() +
#   labs(title = "Sum of precipitations per city and month", y = "precipitations in mm", x = "")
```


## Temperature

Here, I am checking if the na in our weather data temperature are also na on the metesostat.net website.
```{r temperature-na}
# temp_na <- filter(weather_data,is.na(weather_data$temp)) %>% arrange(desc(date))
# length(temp_na$date)
# temp_na %>% group_by(city,date) %>% count() %>% arrange(desc(n))
# head(temp_na,200)
```

There are 1'404 missing values which we could fill with fill(weather_data, temp, .direction = 'down').
I don't know how many bike data are concerned. Another possibility, would be to just remove the bike data with missing temperature.
If we choose to fill missing data, we would need to do it before merging the weather and bike datasets.
Let me know what you guys think.

```{r temperature}
# tempe <- weather_data %>% filter(date > "2017-12-31") %>%
#   #mutate(temp = replace_na(temp, 0)) %>%
#   fill(temp, .direction = 'down') %>%
#   group_by(city, year_month = as.Date(paste(format(date, "%Y-%m"), "-01", sep=""))) %>% 
#   summarise(temp_av = mean(temp)) 
# 
# ggplot(data = tempe, mapping = aes(y = temp_av, x = year_month, group = city, colour = city)) + 
#   geom_point() +
#   geom_line() +
#   labs(title = "Average temperature per city and month", y = "temperature in Â°C", x = "")
```

# Linear Model

# Generalised Linear Model with family set to Poisson

# Generalised Linear Model with family set to Binomial

# Generalised Additive Model

# Neural Network

# Support Vector Machine

We predict the city based on latitude and longitude.

```{r cities_dataset}
# prepare data
cities_start <- data %>% select(c(start_station_latitude, start_station_longitude, city))
colnames(cities_start) <- c("latitude", "longitude","city")

cities_end <- data %>% select(c(end_station_latitude, end_station_longitude, city))
colnames(cities_end) <- c("latitude", "longitude","city")

cities <- rbind(cities_start, cities_end)
rm(cities_start, cities_end)

cities$city <- as.factor(cities$city)

print(colSums(is.na(cities))) # no missing values!

set.seed(10)
cities_subset <- rbind(
  cities[city == 'Trondheim'][sample(nrow(cities[city == 'Trondheim']), 33300), ],
  cities[city == 'Oslo'][sample(nrow(cities[city == 'Oslo']), 33300), ],
  cities[city == 'Bergen'][sample(nrow(cities[city == 'Bergen']), 33300), ]
)
cities_subset %>% 
  group_by(city) %>% 
  summarise('number' = n())
  
```

```{r}
cities_subset %>% 
  ggplot(mapping = aes(x = longitude, y = latitude, color = city)) +
    geom_point() +
    labs(title = "Start/End points per city")
```


```{r cities_dataset_split}
# form training and testing datasets
set.seed(10)
indices <- createDataPartition(cities_subset$city, p=0.7, list=F)

train_cities <- cities_subset %>% slice(indices)
test_cities <-  cities_subset %>% slice(-indices)
test_cities_truth <- cities_subset %>% slice(-indices) %>% pull(city)
```

## Linear kernel

```{r SVM_city_linear}
cities_svm <- svm(city ~. , train_cities, kernel = "linear", scale = TRUE, cost = 10)

#make predictions
test_pred <- predict(cities_svm, test_cities)
table(test_pred)
```

```{r}
plot(cities_svm, train_cities, latitude ~ longitude)
```

```{r}
#evaluate the results
conf_matrix <- confusionMatrix(test_pred, test_cities_truth)
conf_matrix

```

Only 3 data points were misclassified. We built a SVM with a prediction accuracy of nearly 100%.
__cf. Iris lab: We should do some cross-validation to ensure that we are not simply lucky with the train/test split. But on the other end... it is good enough, no?__

## Radial kernel

```{r SVM_city_radial}
cities_svm_radial <- svm(city ~. , train_cities, kernel = "radial", scale = TRUE, cost = 100)

# plot classification
plot(cities_svm_radial, train_cities, latitude ~ longitude)
```

```{r}
#make predictions
test_pred2 <- predict(cities_svm_radial, test_cities)
table(test_pred2)
```

```{r}
#evaluate the results
conf_matrix2 <- confusionMatrix(test_pred2, test_cities_truth)
conf_matrix2
```
Same results with another type of kernel.

__We ignore the 3 wrong values (at least for now :)__ 

# Solve an optimisation problem
